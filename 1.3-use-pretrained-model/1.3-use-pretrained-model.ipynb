{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to use pre-trained models to classify objects in photos #\n",
    "\n",
    "Convolutional neural networks now outperform human eyes on some computer vision tasks, such as image classification.\n",
    "<br><br>\n",
    "That is, given a photo of an object, we can ask the computer to answer the question of which of the 1000 specific types of object this photo is.\n",
    "<br><br>\n",
    "Models for image classification with weights trained on ImageNet:\n",
    "* VGG16\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* InceptionV3\n",
    "* InceptionResNetV2\n",
    "* Xception\n",
    "* MobileNet\n",
    "<br><br>\n",
    "![imagenet](https://imgur.com/am6MnJe.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Windows-10-10.0.16299-SP0\n",
      "Tensorflow version: 1.4.0\n",
      "Keras version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import tensorflow\n",
    "import keras\n",
    "print(\"Platform: {}\".format(platform.platform()))\n",
    "print(\"Tensorflow version: {}\".format(tensorflow.__version__))\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a simple photo classifier #\n",
    "## VGG16 ##\n",
    "### 1.Get the sample image ###\n",
    "\n",
    "First, we need an image that we can categorize.\n",
    "\n",
    "I use:\n",
    "![image](https://imgur.com/mLTMI9Q.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.Load the VGG model\n",
    "Load the weight model file for KeGG's pre-trained VGG-16."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "model_vgg16 = VGG16()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.Load and prepare the image\n",
    "\n",
    "Next, we can load the image in and convert to the tensor specifications required by the pretraining network.\n",
    "\n",
    "Keras provides some tools to help with this step.\n",
    "\n",
    "First, we can load the image using the load_img () function and resize it to the size of 224x224 pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import load_img\n",
    "\n",
    "# load image\n",
    "img_file = 'cat.jpg'\n",
    "image = load_img(img_file, target_size=(224, 224)) # Because the model input for VGG16 is 224x224"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can convert the pixel to a NumPy array so that we can use it in Keras. We can use this img_to_array () function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (224, 224, 3)\n",
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import img_to_array\n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "\n",
    "print(\"image.shape:\", image.shape)\n",
    "print(type(image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "VGG16 networks expect single-color(gray) or multi-color imagery (rgb) as input; this means that the input array needs to be transformed into four dimensions:\n",
    "\n",
    "(Image batch size, image height, image width, image color scale) -> (batch_size, img_height, img_width, img_channels)\n",
    "\n",
    "We have only one sample (one image). We can resize the array by calling reshape () and add additional dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (1, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "# Adjust the dimension of the tensor\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "\n",
    "print(\"image.shape:\", image.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "Next, we need to preprocess the image in the same way that VGG trains ImageNet data. Specifically speaking, from the thesis:\n",
    "\n",
    "> The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\n",
    "\n",
    "Keras provides a function called preprocess_input () to prepare a new image input for the VGG network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import preprocess_input\n",
    "\n",
    "# Prepare the image of the VGG model\n",
    "image = preprocess_input(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.Make a prediction\n",
    "\n",
    "We can call the predict () function in the model to predict the probability that the image will belong to 1000 known object types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Probability of all output categories\n",
    "\n",
    "y_pred = model_vgg16.predict(image)\n",
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.Explain the prediction\n",
    "\n",
    "Keras provides a function to explain the probability called decode_predictions ().\n",
    "\n",
    "It can return a list of categories and the probability of each category, for the sake of simplicity, we will only show the first species of the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('n02124075', 'Egyptian_cat', 0.41167232), ('n02123045', 'tabby', 0.16184787), ('n02123159', 'tiger_cat', 0.14059816), ('n04589890', 'window_screen', 0.06338048), ('n04209239', 'shower_curtain', 0.014743416)]]\n",
      "Egyptian_cat (41.17%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import decode_predictions\n",
    "\n",
    "# Convert probability to category label\n",
    "label = decode_predictions(y_pred)\n",
    "\n",
    "print(label)\n",
    "\n",
    "# Retrieve the most likely result, such as the highest probability\n",
    "label = label[0][0]\n",
    "\n",
    "print(\"{} ({:.2f}%)\".format(label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (224, 224, 3)\n",
      "image.shape: (1, 224, 224, 3)\n",
      "[[('n02124075', 'Egyptian_cat', 0.52090442), ('n02123045', 'tabby', 0.13889943), ('n02342885', 'hamster', 0.10467245), ('n02123159', 'tiger_cat', 0.079205178), ('n02127052', 'lynx', 0.018718716)]]\n",
      "Egyptian_cat (52.09%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input\n",
    "from keras.applications.resnet50 import decode_predictions\n",
    "\n",
    "# Load weight\n",
    "model_resnet50 = ResNet50(weights='imagenet')\n",
    "\n",
    "img_file = 'cat.jpg'\n",
    "image = load_img(img_file, target_size=(224, 224)) \n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "\n",
    "y_pred = model_resnet50.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "print(label)\n",
    "\n",
    "label = label[0][0]\n",
    "\n",
    "print(\"{} ({:.2f}%)\".format(label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionV3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (299, 299, 3)\n",
      "image.shape: (1, 299, 299, 3)\n",
      "[[('n02123159', 'tiger_cat', 0.53256637), ('n02124075', 'Egyptian_cat', 0.25048947), ('n02123045', 'tabby', 0.12913629), ('n02127052', 'lynx', 0.011345633), ('n02971356', 'carton', 0.0025923138)]]\n",
      "tiger_cat (53.26%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_v3 import preprocess_input\n",
    "from keras.applications.inception_v3 import decode_predictions\n",
    "\n",
    "model_inception_v3 = InceptionV3(weights='imagenet')\n",
    "img_file = 'cat.jpg'\n",
    "\n",
    "# The input for the model of InceptionV3 is 299x299\n",
    "image = load_img(img_file, target_size=(299, 299)) \n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "y_pred = model_inception_v3.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "print(label)\n",
    "\n",
    "label = label[0][0]\n",
    "\n",
    "print(\"{} ({:.2f}%)\".format(label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InceptionResNetV2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image.shape: (299, 299, 3)\n",
      "image.shape: (1, 299, 299, 3)\n",
      "[[('n02123159', 'tiger_cat', 0.50081289), ('n02123045', 'tabby', 0.35746354), ('n02124075', 'Egyptian_cat', 0.061717406), ('n02127052', 'lynx', 0.00906057), ('n03657121', 'lens_cap', 0.00097865588)]]\n",
      "tiger_cat (50.08%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.inception_resnet_v2 import InceptionResNetV2\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.inception_resnet_v2 import preprocess_input\n",
    "from keras.applications.inception_resnet_v2 import decode_predictions\n",
    "\n",
    "model_inception_resnet_v2 = InceptionResNetV2(weights='imagenet')\n",
    "img_file = 'cat.jpg'\n",
    "\n",
    "# The model for InceptionResNetV2 has an input of 299x299\n",
    "image = load_img(img_file, target_size=(299, 299)) \n",
    "\n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "y_pred = model_inception_resnet_v2.predict(image)\n",
    "label = decode_predictions(y_pred)\n",
    "print(label)\n",
    "\n",
    "label = label[0][0]\n",
    "print(\"{} ({:.2f}%)\".format(label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MobileNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://github.com/fchollet/deep-learning-models/releases/download/v0.6/mobilenet_1_0_224_tf.h5\n",
      "17227776/17225924 [==============================] - 6s 0us/step\n",
      "image.shape: (224, 224, 3)\n",
      "image.shape: (1, 224, 224, 3)\n",
      "[[('n02123045', 'tabby', 0.34371877), ('n02124075', 'Egyptian_cat', 0.31817058), ('n02123159', 'tiger_cat', 0.26132542), ('n02127052', 'lynx', 0.015763907), ('n03657121', 'lens_cap', 0.01001161)]]\n",
      "tabby (34.37%)\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.mobilenet import MobileNet\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.mobilenet import preprocess_input\n",
    "from keras.applications.mobilenet import decode_predictions\n",
    "\n",
    "model_mobilenet = MobileNet(weights='imagenet')\n",
    "img_file = 'cat.jpg'\n",
    "\n",
    "image = load_img(img_file, target_size=(224, 224)) \n",
    "image = img_to_array(image) # RGB\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "print(\"image.shape:\", image.shape)\n",
    "\n",
    "image = preprocess_input(image)\n",
    "y_pred = model_mobilenet.predict(image)\n",
    "\n",
    "label = decode_predictions(y_pred)\n",
    "print(label)\n",
    "\n",
    "label = label[0][0]\n",
    "print(\"{} ({:.2f}%)\".format(label[1], label[2]*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "Need to understand the structure and input tensors for each type of network identified by advanced image recognition\n",
    "\n",
    "Understanding the amount of training variable and pre-training weights for different advanced image recognition networks can effectively help with image recognition type tasks\n",
    "\n",
    "<br><br><br>\n",
    "Reference:\n",
    "* [How to Use The Pre-Trained VGG Model to Classify Objects in Photographs](https://machinelearningmastery.com/use-pre-trained-vgg-model-classify-objects-photographs/)\n",
    "* [Keras Available models](https://keras.io/applications/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
