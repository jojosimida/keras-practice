{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is sequence-to-sequence learning?\n",
    "\n",
    "Sequence-to-sequence learning (Seq2Seq) is about training models to convert sequences from one domain (e.g. sentences in English) to sequences in another domain (e.g. the same sentences translated to French).\n",
    "\n",
    "![ex](https://imgur.com/XwREty3.jpg)\n",
    "\n",
    "      \"the cat sat on the mat\" -> [Seq2Seq model] -> \"那隻貓坐在地毯上\" \n",
    "      \n",
    "This can be used for machine translation or for free-from question answering (generating a natural language answer given a natural language question) -- in general, it is applicable any time you need to generate text.\n",
    "\n",
    "There are multiple ways to handle this task, either using RNNs or using 1D convnets. Here we will focus on RNNs.\n",
    "\n",
    "## The trivial case: when input and output sequences have the same length\n",
    "\n",
    "When both input sequences and output sequences have the same length, you can implement such models simply with a Keras LSTM or GRU layer (or stack thereof). This is the case in this example script that shows how to teach a RNN to learn to add numbers, encoded as character strings:\n",
    "\n",
    "![LSTM](https://blog.keras.io/img/seq2seq/addition-rnn.png)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras import layers\n",
    "from keras.utils import plot_model\n",
    "import numpy as np\n",
    "from six.moves import range\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CharacterTable(object):\n",
    "    \"\"\"\n",
    "     Give a group of characters:\n",
    "     + Encode these characters using one-hot encoding into numbers\n",
    "     + Decode one-hot encoded digits to be the original character\n",
    "     + Decode the probability of a character to answer the most likely character\n",
    "    \"\"\"\n",
    "    def __init__(self, chars):\n",
    "        \"\"\"\n",
    "        Initialize character table\n",
    "        \n",
    "         # Parameters:\n",
    "             chars: Appears in the possible character set entered\n",
    "        \"\"\"\n",
    "        self.chars = sorted(set(chars))\n",
    "        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))\n",
    "        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))\n",
    "        \n",
    "    def encode(self, C, num_rows):\n",
    "        \"\"\"\n",
    "        Enter the string one-hot encoding\n",
    "        \n",
    "         # Parameters:\n",
    "             C: The character to be encoded\n",
    "             num_rows: The maximum number of lines to be returned after one-hot encoding. \n",
    "                       This is to make sure that every input is there\n",
    "                       The same number of lines of output\n",
    "        \"\"\"\n",
    "        x = np.zeros((num_rows, len(self.chars)))\n",
    "        for i, c in enumerate(C):\n",
    "            x[i, self.char_indices[c]] = 1\n",
    "        return x\n",
    "    \n",
    "    def decode(self, x, calc_argmax=True):\n",
    "        \"\"\"\n",
    "        The input code (vector) is decoded\n",
    "        \n",
    "         # Parameters:\n",
    "             x: character vector or character encoding to be decoded\n",
    "             calc_argmax: Whether to use the argmax operator to find the most likely character encoding\n",
    "        \"\"\"\n",
    "        if calc_argmax:\n",
    "            x = x.argmax(axis=-1)\n",
    "        return ''.join(self.indices_char[x] for x in x)\n",
    "    \n",
    "class colors:\n",
    "    ok = '\\033[92m'\n",
    "    fail = '\\033[91m'\n",
    "    close = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevant parameters and training data set generated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating data...\n",
      "Total addition questions: 50000\n"
     ]
    }
   ],
   "source": [
    "# Model and data set parameters\n",
    "TRAINING_SIZE = 50000 \n",
    "DIGITS = 3            \n",
    "INVERT = True \n",
    "\n",
    "# the maximum length of enter: 'int + int' (ex, '345+678')\n",
    "MAXLEN = DIGITS + 1 + DIGITS\n",
    "\n",
    "# All characters to use (including numbers, plus signs and spaces)\n",
    "chars = '0123456789+ '\n",
    "# Create CharacterTable instance\n",
    "ctable = CharacterTable(chars) \n",
    "\n",
    "# Training sentence \"xxx + yyy\"\n",
    "questions = [] \n",
    "# Training label\n",
    "expected = []  \n",
    "seen = set()\n",
    "\n",
    "print('Generating data...')\n",
    "\n",
    "while len(questions) < TRAINING_SIZE:\n",
    "    # Number Generator (3 characters)\n",
    "    f = lambda: int(''.join(np.random.choice(list('0123456789'))\n",
    "                           for i in range(np.random.randint(1, DIGITS+1))))\n",
    "    a, b = f(), f()\n",
    "    \n",
    "    # Skip the topics that have been seen and x + Y = Y + x this problem\n",
    "    key = tuple(sorted((a, b)))\n",
    "    if key in seen:\n",
    "        continue    \n",
    "    seen.add(key)\n",
    "    \n",
    "    # When the number is less than MAXLEN then fill the blank\n",
    "    q = '{}+{}'.format(a, b)\n",
    "    query = q + ' ' * (MAXLEN - len(q))\n",
    "    ans = str(a + b)\n",
    "    \n",
    "    # The maximum character length of the answer is DIGITS + 1\n",
    "    ans += ' ' * (DIGITS + 1 - len(ans))\n",
    "    \n",
    "    if INVERT:\n",
    "        # To reverse the direction of the problem character, eg '12 +345 'becomes' 543 + 21'\n",
    "        query = query[::-1]\n",
    "    questions.append(query)\n",
    "    expected.append(ans)\n",
    "    \n",
    "print('Total addition questions:', len(questions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization...\n",
      "Feature data:  (50000, 7, 12)\n",
      "Label data:  (50000, 4, 12)\n",
      "Training Data:\n",
      "(45000, 7, 12)\n",
      "(45000, 4, 12)\n",
      "Validation Data:\n",
      "(5000, 7, 12)\n",
      "(5000, 4, 12)\n"
     ]
    }
   ],
   "source": [
    "# The appropriate conversion of data, LSTM expected data structure -> [samples, timesteps, features]\n",
    "print('Vectorization...')\n",
    "\n",
    "# The initial three-dimensional numpy ndarray (characteristic data)\n",
    "x = np.zeros((len(questions), MAXLEN, len(chars)), dtype=np.bool) \n",
    "# Initially a 3-D numpy ndarray (label information)\n",
    "y = np.zeros((len(questions), DIGITS + 1, len(chars)), dtype=np.bool) \n",
    "\n",
    "# Convert \"feature data\" into the LSTM's expected data structure -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(questions):\n",
    "    x[i] = ctable.encode(sentence, MAXLEN)      \n",
    "\n",
    "print(\"Feature data: \", x.shape)\n",
    "\n",
    "# Convert \"label data\" into the LSTM's expected data structure -> [samples, timesteps, features]\n",
    "for i, sentence in enumerate(expected):\n",
    "    y[i] = ctable.encode(sentence, DIGITS + 1)  \n",
    "\n",
    "print(\"Label data: \", y.shape)\n",
    "\n",
    "# Shuffle(x, y)\n",
    "indices = np.arange(len(y))\n",
    "np.random.shuffle(indices)\n",
    "x = x[indices]\n",
    "y = y[indices]\n",
    "\n",
    "# Retain 10% of the information for verification\n",
    "split_at = len(x) - len(x) // 10\n",
    "(x_train, x_val) = x[:split_at], x[split_at:]\n",
    "(y_train, y_val) = y[:split_at], y[split_at:]\n",
    "\n",
    "print('Training Data:')\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "\n",
    "print('Validation Data:')\n",
    "print(x_val.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a network infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               72192     \n",
      "_________________________________________________________________\n",
      "repeat_vector_1 (RepeatVecto (None, 4, 128)            0         \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 4, 128)            131584    \n",
      "_________________________________________________________________\n",
      "time_distributed_1 (TimeDist (None, 4, 12)             1548      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 4, 12)             0         \n",
      "=================================================================\n",
      "Total params: 205,324\n",
      "Trainable params: 205,324\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Try to replace other rnn units, such as GRU or SimpleRNN\n",
    "RNN = layers.LSTM\n",
    "HIDDEN_SIZE = 128\n",
    "BATCH_SIZE = 128\n",
    "LAYERS = 1\n",
    "\n",
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# ===== encoder ====\n",
    "\n",
    "# Generate the output of HIDDEN_SIZE using the RNN \"code\" input sequence.\n",
    "# Note: With input sequence length variable, use input_shape = (None, num_features)\n",
    "\n",
    "# MAXLEN stands for timesteps, len (chars) is one-hot-coded features\n",
    "model.add(RNN(HIDDEN_SIZE, input_shape=(MAXLEN, len(chars)))) \n",
    "\n",
    "# As input to the decoder RNN, the last hidden state of the RNN providing each time step is repeated.\n",
    "# Repeat \"DIGITS + 1\" times because this is the maximum output length, for example, when DIGITS = 3, \n",
    "# the maximum output is 999 + 999 = 1998 (length 4).\n",
    "model.add(layers.RepeatVector(DIGITS+1))\n",
    "\n",
    "\n",
    "# ==== decoder ====\n",
    "# The decoder RNNs can be multi-layer stacks or single layers.\n",
    "for _ in range(LAYERS):\n",
    "    # By setting return_sequences to True, not only the last output is returned, but also all outputs are returned as \n",
    "    # (num_samples, timesteps, output_dim). This is necessary because the following TimeDistributed requires that the \n",
    "    # first dimension be a time step.\n",
    "    model.add(RNN(HIDDEN_SIZE, return_sequences=True))\n",
    "\n",
    "# Each entered time slice is pushed to the dense layer to decide which character to select \n",
    "# for each time step of the output sequence.\n",
    "model.add(layers.TimeDistributed(layers.Dense(len(chars))))\n",
    "\n",
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "             optimizer='adam',\n",
    "             metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Model / Verification Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--------------------------------------------------\n",
      "Iteration 1\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 13s 293us/step - loss: 1.8843 - acc: 0.3219 - val_loss: 1.7856 - val_acc: 0.3355\n",
      "Q 90+51   T 141  \u001b[91m☒\u001b[0m 161 \n",
      "Q 43+263  T 306  \u001b[91m☒\u001b[0m 161 \n",
      "Q 901+897 T 1798 \u001b[91m☒\u001b[0m 1014\n",
      "Q 69+94   T 163  \u001b[91m☒\u001b[0m 166 \n",
      "Q 331+913 T 1244 \u001b[91m☒\u001b[0m 101 \n",
      "Q 622+71  T 693  \u001b[91m☒\u001b[0m 161 \n",
      "Q 974+45  T 1019 \u001b[91m☒\u001b[0m 101 \n",
      "Q 9+770   T 779  \u001b[91m☒\u001b[0m 101 \n",
      "Q 123+209 T 332  \u001b[91m☒\u001b[0m 101 \n",
      "Q 44+162  T 206  \u001b[91m☒\u001b[0m 161 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 2\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 177us/step - loss: 1.7069 - acc: 0.3672 - val_loss: 1.6326 - val_acc: 0.3935\n",
      "Q 673+2   T 675  \u001b[91m☒\u001b[0m 171 \n",
      "Q 57+69   T 126  \u001b[91m☒\u001b[0m 171 \n",
      "Q 658+544 T 1202 \u001b[91m☒\u001b[0m 1207\n",
      "Q 1+508   T 509  \u001b[91m☒\u001b[0m 111 \n",
      "Q 41+989  T 1030 \u001b[91m☒\u001b[0m 900 \n",
      "Q 8+769   T 777  \u001b[91m☒\u001b[0m 171 \n",
      "Q 608+61  T 669  \u001b[91m☒\u001b[0m 771 \n",
      "Q 175+472 T 647  \u001b[91m☒\u001b[0m 711 \n",
      "Q 641+637 T 1278 \u001b[91m☒\u001b[0m 1277\n",
      "Q 44+382  T 426  \u001b[91m☒\u001b[0m 441 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 3\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 166us/step - loss: 1.5537 - acc: 0.4193 - val_loss: 1.4666 - val_acc: 0.4517\n",
      "Q 989+960 T 1949 \u001b[91m☒\u001b[0m 1710\n",
      "Q 74+406  T 480  \u001b[91m☒\u001b[0m 444 \n",
      "Q 629+19  T 648  \u001b[91m☒\u001b[0m 661 \n",
      "Q 689+419 T 1108 \u001b[91m☒\u001b[0m 1004\n",
      "Q 333+9   T 342  \u001b[91m☒\u001b[0m 334 \n",
      "Q 105+255 T 360  \u001b[91m☒\u001b[0m 574 \n",
      "Q 403+5   T 408  \u001b[91m☒\u001b[0m 440 \n",
      "Q 920+890 T 1810 \u001b[91m☒\u001b[0m 1780\n",
      "Q 840+969 T 1809 \u001b[91m☒\u001b[0m 1670\n",
      "Q 529+58  T 587  \u001b[91m☒\u001b[0m 560 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 4\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 156us/step - loss: 1.3894 - acc: 0.4828 - val_loss: 1.3127 - val_acc: 0.5121\n",
      "Q 54+599  T 653  \u001b[91m☒\u001b[0m 556 \n",
      "Q 111+73  T 184  \u001b[91m☒\u001b[0m 111 \n",
      "Q 785+49  T 834  \u001b[91m☒\u001b[0m 883 \n",
      "Q 870+812 T 1682 \u001b[91m☒\u001b[0m 1788\n",
      "Q 44+2    T 46   \u001b[91m☒\u001b[0m 48  \n",
      "Q 883+23  T 906  \u001b[91m☒\u001b[0m 898 \n",
      "Q 741+3   T 744  \u001b[91m☒\u001b[0m 747 \n",
      "Q 404+107 T 511  \u001b[91m☒\u001b[0m 486 \n",
      "Q 383+952 T 1335 \u001b[91m☒\u001b[0m 1377\n",
      "Q 780+409 T 1189 \u001b[91m☒\u001b[0m 1177\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 5\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 154us/step - loss: 1.2560 - acc: 0.5360 - val_loss: 1.2362 - val_acc: 0.5401\n",
      "Q 926+3   T 929  \u001b[91m☒\u001b[0m 920 \n",
      "Q 32+853  T 885  \u001b[91m☒\u001b[0m 867 \n",
      "Q 654+14  T 668  \u001b[91m☒\u001b[0m 661 \n",
      "Q 49+978  T 1027 \u001b[92m☑\u001b[0m 1027\n",
      "Q 30+957  T 987  \u001b[91m☒\u001b[0m 977 \n",
      "Q 539+31  T 570  \u001b[91m☒\u001b[0m 541 \n",
      "Q 75+708  T 783  \u001b[91m☒\u001b[0m 777 \n",
      "Q 90+703  T 793  \u001b[91m☒\u001b[0m 777 \n",
      "Q 528+78  T 606  \u001b[91m☒\u001b[0m 593 \n",
      "Q 857+737 T 1594 \u001b[91m☒\u001b[0m 1581\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 6\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 155us/step - loss: 1.1422 - acc: 0.5803 - val_loss: 1.0964 - val_acc: 0.6010\n",
      "Q 971+34  T 1005 \u001b[91m☒\u001b[0m 100 \n",
      "Q 465+985 T 1450 \u001b[91m☒\u001b[0m 1541\n",
      "Q 429+15  T 444  \u001b[91m☒\u001b[0m 449 \n",
      "Q 615+887 T 1502 \u001b[91m☒\u001b[0m 1591\n",
      "Q 80+67   T 147  \u001b[91m☒\u001b[0m 140 \n",
      "Q 743+982 T 1725 \u001b[91m☒\u001b[0m 1694\n",
      "Q 304+26  T 330  \u001b[91m☒\u001b[0m 337 \n",
      "Q 71+839  T 910  \u001b[92m☑\u001b[0m 910 \n",
      "Q 21+450  T 471  \u001b[91m☒\u001b[0m 469 \n",
      "Q 13+415  T 428  \u001b[91m☒\u001b[0m 444 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 7\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 156us/step - loss: 1.0401 - acc: 0.6216 - val_loss: 1.0163 - val_acc: 0.6304\n",
      "Q 993+54  T 1047 \u001b[91m☒\u001b[0m 1043\n",
      "Q 366+49  T 415  \u001b[91m☒\u001b[0m 404 \n",
      "Q 937+0   T 937  \u001b[91m☒\u001b[0m 934 \n",
      "Q 636+802 T 1438 \u001b[91m☒\u001b[0m 1474\n",
      "Q 88+731  T 819  \u001b[91m☒\u001b[0m 828 \n",
      "Q 15+115  T 130  \u001b[91m☒\u001b[0m 137 \n",
      "Q 43+898  T 941  \u001b[91m☒\u001b[0m 975 \n",
      "Q 653+27  T 680  \u001b[91m☒\u001b[0m 681 \n",
      "Q 1+88    T 89   \u001b[91m☒\u001b[0m 88  \n",
      "Q 730+583 T 1313 \u001b[91m☒\u001b[0m 1327\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 8\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 153us/step - loss: 0.9602 - acc: 0.6536 - val_loss: 0.9237 - val_acc: 0.6709\n",
      "Q 59+749  T 808  \u001b[91m☒\u001b[0m 810 \n",
      "Q 29+376  T 405  \u001b[91m☒\u001b[0m 418 \n",
      "Q 7+995   T 1002 \u001b[91m☒\u001b[0m 1000\n",
      "Q 936+746 T 1682 \u001b[91m☒\u001b[0m 1681\n",
      "Q 959+566 T 1525 \u001b[91m☒\u001b[0m 1522\n",
      "Q 777+73  T 850  \u001b[91m☒\u001b[0m 842 \n",
      "Q 861+793 T 1654 \u001b[91m☒\u001b[0m 1644\n",
      "Q 724+267 T 991  \u001b[91m☒\u001b[0m 908 \n",
      "Q 231+217 T 448  \u001b[91m☒\u001b[0m 458 \n",
      "Q 252+555 T 807  \u001b[91m☒\u001b[0m 788 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 9\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 153us/step - loss: 0.8817 - acc: 0.6840 - val_loss: 0.8584 - val_acc: 0.6862\n",
      "Q 91+46   T 137  \u001b[91m☒\u001b[0m 134 \n",
      "Q 564+902 T 1466 \u001b[91m☒\u001b[0m 1480\n",
      "Q 824+305 T 1129 \u001b[91m☒\u001b[0m 1144\n",
      "Q 555+82  T 637  \u001b[92m☑\u001b[0m 637 \n",
      "Q 798+561 T 1359 \u001b[91m☒\u001b[0m 1354\n",
      "Q 49+954  T 1003 \u001b[91m☒\u001b[0m 1002\n",
      "Q 860+92  T 952  \u001b[92m☑\u001b[0m 952 \n",
      "Q 280+51  T 331  \u001b[91m☒\u001b[0m 324 \n",
      "Q 12+830  T 842  \u001b[91m☒\u001b[0m 847 \n",
      "Q 16+176  T 192  \u001b[91m☒\u001b[0m 187 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 10\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 159us/step - loss: 0.8098 - acc: 0.7117 - val_loss: 0.7960 - val_acc: 0.7138\n",
      "Q 160+4   T 164  \u001b[92m☑\u001b[0m 164 \n",
      "Q 60+341  T 401  \u001b[91m☒\u001b[0m 498 \n",
      "Q 202+0   T 202  \u001b[92m☑\u001b[0m 202 \n",
      "Q 45+800  T 845  \u001b[91m☒\u001b[0m 844 \n",
      "Q 77+950  T 1027 \u001b[91m☒\u001b[0m 1021\n",
      "Q 652+17  T 669  \u001b[91m☒\u001b[0m 678 \n",
      "Q 3+916   T 919  \u001b[91m☒\u001b[0m 918 \n",
      "Q 114+510 T 624  \u001b[91m☒\u001b[0m 638 \n",
      "Q 16+87   T 103  \u001b[91m☒\u001b[0m 10  \n",
      "Q 167+879 T 1046 \u001b[91m☒\u001b[0m 1040\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 11\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 173us/step - loss: 0.7506 - acc: 0.7340 - val_loss: 0.7418 - val_acc: 0.7358\n",
      "Q 28+858  T 886  \u001b[91m☒\u001b[0m 880 \n",
      "Q 34+21   T 55   \u001b[91m☒\u001b[0m 53  \n",
      "Q 193+23  T 216  \u001b[91m☒\u001b[0m 210 \n",
      "Q 500+974 T 1474 \u001b[91m☒\u001b[0m 1470\n",
      "Q 13+661  T 674  \u001b[91m☒\u001b[0m 677 \n",
      "Q 14+18   T 32   \u001b[91m☒\u001b[0m 39  \n",
      "Q 96+735  T 831  \u001b[91m☒\u001b[0m 820 \n",
      "Q 85+998  T 1083 \u001b[91m☒\u001b[0m 1086\n",
      "Q 32+853  T 885  \u001b[91m☒\u001b[0m 880 \n",
      "Q 196+72  T 268  \u001b[91m☒\u001b[0m 264 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 12\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 159us/step - loss: 0.6824 - acc: 0.7592 - val_loss: 0.6379 - val_acc: 0.7794\n",
      "Q 370+609 T 979  \u001b[91m☒\u001b[0m 978 \n",
      "Q 857+832 T 1689 \u001b[91m☒\u001b[0m 1680\n",
      "Q 677+673 T 1350 \u001b[91m☒\u001b[0m 1355\n",
      "Q 699+568 T 1267 \u001b[91m☒\u001b[0m 1271\n",
      "Q 398+4   T 402  \u001b[91m☒\u001b[0m 401 \n",
      "Q 50+234  T 284  \u001b[92m☑\u001b[0m 284 \n",
      "Q 873+716 T 1589 \u001b[91m☒\u001b[0m 1582\n",
      "Q 415+1   T 416  \u001b[92m☑\u001b[0m 416 \n",
      "Q 669+74  T 743  \u001b[91m☒\u001b[0m 742 \n",
      "Q 84+588  T 672  \u001b[91m☒\u001b[0m 668 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 13\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 155us/step - loss: 0.5592 - acc: 0.8040 - val_loss: 0.4679 - val_acc: 0.8356\n",
      "Q 62+209  T 271  \u001b[92m☑\u001b[0m 271 \n",
      "Q 587+95  T 682  \u001b[91m☒\u001b[0m 671 \n",
      "Q 71+83   T 154  \u001b[91m☒\u001b[0m 156 \n",
      "Q 536+467 T 1003 \u001b[91m☒\u001b[0m 1002\n",
      "Q 479+2   T 481  \u001b[92m☑\u001b[0m 481 \n",
      "Q 51+97   T 148  \u001b[91m☒\u001b[0m 149 \n",
      "Q 94+541  T 635  \u001b[91m☒\u001b[0m 634 \n",
      "Q 897+7   T 904  \u001b[92m☑\u001b[0m 904 \n",
      "Q 270+17  T 287  \u001b[92m☑\u001b[0m 287 \n",
      "Q 583+134 T 717  \u001b[92m☑\u001b[0m 717 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 14\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 156us/step - loss: 0.3793 - acc: 0.8836 - val_loss: 0.3265 - val_acc: 0.9101\n",
      "Q 80+689  T 769  \u001b[91m☒\u001b[0m 778 \n",
      "Q 843+146 T 989  \u001b[91m☒\u001b[0m 988 \n",
      "Q 843+146 T 989  \u001b[91m☒\u001b[0m 988 \n",
      "Q 0+344   T 344  \u001b[92m☑\u001b[0m 344 \n",
      "Q 6+361   T 367  \u001b[92m☑\u001b[0m 367 \n",
      "Q 232+85  T 317  \u001b[92m☑\u001b[0m 317 \n",
      "Q 12+881  T 893  \u001b[92m☑\u001b[0m 893 \n",
      "Q 122+99  T 221  \u001b[92m☑\u001b[0m 221 \n",
      "Q 306+1   T 307  \u001b[92m☑\u001b[0m 307 \n",
      "Q 42+24   T 66   \u001b[92m☑\u001b[0m 66  \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 15\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 7s 152us/step - loss: 0.2685 - acc: 0.9333 - val_loss: 0.2541 - val_acc: 0.9279\n",
      "Q 702+86  T 788  \u001b[92m☑\u001b[0m 788 \n",
      "Q 911+744 T 1655 \u001b[92m☑\u001b[0m 1655\n",
      "Q 614+83  T 697  \u001b[92m☑\u001b[0m 697 \n",
      "Q 220+48  T 268  \u001b[92m☑\u001b[0m 268 \n",
      "Q 745+89  T 834  \u001b[92m☑\u001b[0m 834 \n",
      "Q 317+86  T 403  \u001b[92m☑\u001b[0m 403 \n",
      "Q 4+198   T 202  \u001b[92m☑\u001b[0m 202 \n",
      "Q 767+9   T 776  \u001b[92m☑\u001b[0m 776 \n",
      "Q 361+473 T 834  \u001b[92m☑\u001b[0m 834 \n",
      "Q 446+4   T 450  \u001b[92m☑\u001b[0m 450 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 16\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 163us/step - loss: 0.1902 - acc: 0.9617 - val_loss: 0.1783 - val_acc: 0.9614\n",
      "Q 638+2   T 640  \u001b[92m☑\u001b[0m 640 \n",
      "Q 147+505 T 652  \u001b[92m☑\u001b[0m 652 \n",
      "Q 15+115  T 130  \u001b[91m☒\u001b[0m 120 \n",
      "Q 187+599 T 786  \u001b[92m☑\u001b[0m 786 \n",
      "Q 103+41  T 144  \u001b[91m☒\u001b[0m 143 \n",
      "Q 928+8   T 936  \u001b[92m☑\u001b[0m 936 \n",
      "Q 964+9   T 973  \u001b[92m☑\u001b[0m 973 \n",
      "Q 43+39   T 82   \u001b[91m☒\u001b[0m 81  \n",
      "Q 96+735  T 831  \u001b[92m☑\u001b[0m 831 \n",
      "Q 9+853   T 862  \u001b[92m☑\u001b[0m 862 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 17\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 169us/step - loss: 0.1432 - acc: 0.9737 - val_loss: 0.1231 - val_acc: 0.9791\n",
      "Q 2+425   T 427  \u001b[92m☑\u001b[0m 427 \n",
      "Q 30+164  T 194  \u001b[92m☑\u001b[0m 194 \n",
      "Q 267+113 T 380  \u001b[92m☑\u001b[0m 380 \n",
      "Q 926+3   T 929  \u001b[92m☑\u001b[0m 929 \n",
      "Q 113+31  T 144  \u001b[92m☑\u001b[0m 144 \n",
      "Q 481+22  T 503  \u001b[92m☑\u001b[0m 503 \n",
      "Q 8+392   T 400  \u001b[91m☒\u001b[0m 300 \n",
      "Q 47+362  T 409  \u001b[92m☑\u001b[0m 409 \n",
      "Q 651+28  T 679  \u001b[92m☑\u001b[0m 679 \n",
      "Q 920+890 T 1810 \u001b[92m☑\u001b[0m 1810\n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 18\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 9s 200us/step - loss: 0.1129 - acc: 0.9803 - val_loss: 0.1002 - val_acc: 0.9820\n",
      "Q 5+408   T 413  \u001b[92m☑\u001b[0m 413 \n",
      "Q 193+699 T 892  \u001b[91m☒\u001b[0m 882 \n",
      "Q 338+114 T 452  \u001b[92m☑\u001b[0m 452 \n",
      "Q 79+809  T 888  \u001b[92m☑\u001b[0m 888 \n",
      "Q 261+447 T 708  \u001b[92m☑\u001b[0m 708 \n",
      "Q 27+68   T 95   \u001b[92m☑\u001b[0m 95  \n",
      "Q 634+293 T 927  \u001b[92m☑\u001b[0m 927 \n",
      "Q 69+19   T 88   \u001b[91m☒\u001b[0m 87  \n",
      "Q 811+71  T 882  \u001b[92m☑\u001b[0m 882 \n",
      "Q 4+327   T 331  \u001b[92m☑\u001b[0m 331 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 19\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 9s 196us/step - loss: 0.0814 - acc: 0.9889 - val_loss: 0.0813 - val_acc: 0.9853\n",
      "Q 916+57  T 973  \u001b[92m☑\u001b[0m 973 \n",
      "Q 76+13   T 89   \u001b[92m☑\u001b[0m 89  \n",
      "Q 929+317 T 1246 \u001b[92m☑\u001b[0m 1246\n",
      "Q 217+1   T 218  \u001b[92m☑\u001b[0m 218 \n",
      "Q 28+953  T 981  \u001b[92m☑\u001b[0m 981 \n",
      "Q 34+619  T 653  \u001b[92m☑\u001b[0m 653 \n",
      "Q 712+62  T 774  \u001b[92m☑\u001b[0m 774 \n",
      "Q 381+15  T 396  \u001b[92m☑\u001b[0m 396 \n",
      "Q 884+6   T 890  \u001b[92m☑\u001b[0m 890 \n",
      "Q 238+75  T 313  \u001b[92m☑\u001b[0m 313 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 20\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 188us/step - loss: 0.0749 - acc: 0.9871 - val_loss: 0.0600 - val_acc: 0.9918\n",
      "Q 459+777 T 1236 \u001b[92m☑\u001b[0m 1236\n",
      "Q 841+12  T 853  \u001b[92m☑\u001b[0m 853 \n",
      "Q 641+669 T 1310 \u001b[92m☑\u001b[0m 1310\n",
      "Q 101+131 T 232  \u001b[92m☑\u001b[0m 232 \n",
      "Q 442+88  T 530  \u001b[92m☑\u001b[0m 530 \n",
      "Q 92+122  T 214  \u001b[92m☑\u001b[0m 214 \n",
      "Q 0+647   T 647  \u001b[92m☑\u001b[0m 647 \n",
      "Q 988+912 T 1900 \u001b[92m☑\u001b[0m 1900\n",
      "Q 613+262 T 875  \u001b[92m☑\u001b[0m 875 \n",
      "Q 49+181  T 230  \u001b[92m☑\u001b[0m 230 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 21\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 170us/step - loss: 0.0510 - acc: 0.9942 - val_loss: 0.0547 - val_acc: 0.9923\n",
      "Q 502+54  T 556  \u001b[92m☑\u001b[0m 556 \n",
      "Q 743+286 T 1029 \u001b[92m☑\u001b[0m 1029\n",
      "Q 6+453   T 459  \u001b[92m☑\u001b[0m 459 \n",
      "Q 41+26   T 67   \u001b[92m☑\u001b[0m 67  \n",
      "Q 971+34  T 1005 \u001b[92m☑\u001b[0m 1005\n",
      "Q 206+87  T 293  \u001b[92m☑\u001b[0m 293 \n",
      "Q 317+86  T 403  \u001b[92m☑\u001b[0m 403 \n",
      "Q 40+417  T 457  \u001b[92m☑\u001b[0m 457 \n",
      "Q 826+5   T 831  \u001b[92m☑\u001b[0m 831 \n",
      "Q 622+54  T 676  \u001b[92m☑\u001b[0m 676 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 22\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 157us/step - loss: 0.0465 - acc: 0.9939 - val_loss: 0.0720 - val_acc: 0.9821\n",
      "Q 6+289   T 295  \u001b[91m☒\u001b[0m 296 \n",
      "Q 66+404  T 470  \u001b[92m☑\u001b[0m 470 \n",
      "Q 387+50  T 437  \u001b[92m☑\u001b[0m 437 \n",
      "Q 439+66  T 505  \u001b[92m☑\u001b[0m 505 \n",
      "Q 387+577 T 964  \u001b[92m☑\u001b[0m 964 \n",
      "Q 995+17  T 1012 \u001b[92m☑\u001b[0m 1012\n",
      "Q 60+945  T 1005 \u001b[92m☑\u001b[0m 1005\n",
      "Q 712+37  T 749  \u001b[92m☑\u001b[0m 749 \n",
      "Q 401+34  T 435  \u001b[92m☑\u001b[0m 435 \n",
      "Q 937+0   T 937  \u001b[92m☑\u001b[0m 937 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 23\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 161us/step - loss: 0.0405 - acc: 0.9944 - val_loss: 0.0344 - val_acc: 0.9959\n",
      "Q 33+28   T 61   \u001b[92m☑\u001b[0m 61  \n",
      "Q 821+21  T 842  \u001b[92m☑\u001b[0m 842 \n",
      "Q 890+359 T 1249 \u001b[92m☑\u001b[0m 1249\n",
      "Q 148+618 T 766  \u001b[92m☑\u001b[0m 766 \n",
      "Q 88+918  T 1006 \u001b[92m☑\u001b[0m 1006\n",
      "Q 2+630   T 632  \u001b[92m☑\u001b[0m 632 \n",
      "Q 448+966 T 1414 \u001b[92m☑\u001b[0m 1414\n",
      "Q 0+81    T 81   \u001b[91m☒\u001b[0m 82  \n",
      "Q 832+20  T 852  \u001b[92m☑\u001b[0m 852 \n",
      "Q 571+43  T 614  \u001b[92m☑\u001b[0m 614 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 24\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 161us/step - loss: 0.0290 - acc: 0.9972 - val_loss: 0.0343 - val_acc: 0.9945\n",
      "Q 898+76  T 974  \u001b[92m☑\u001b[0m 974 \n",
      "Q 193+82  T 275  \u001b[92m☑\u001b[0m 275 \n",
      "Q 816+7   T 823  \u001b[92m☑\u001b[0m 823 \n",
      "Q 160+881 T 1041 \u001b[92m☑\u001b[0m 1041\n",
      "Q 48+13   T 61   \u001b[92m☑\u001b[0m 61  \n",
      "Q 34+619  T 653  \u001b[92m☑\u001b[0m 653 \n",
      "Q 231+366 T 597  \u001b[92m☑\u001b[0m 597 \n",
      "Q 512+564 T 1076 \u001b[92m☑\u001b[0m 1076\n",
      "Q 34+935  T 969  \u001b[92m☑\u001b[0m 969 \n",
      "Q 27+356  T 383  \u001b[92m☑\u001b[0m 383 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 25\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 168us/step - loss: 0.0403 - acc: 0.9921 - val_loss: 0.2160 - val_acc: 0.9394\n",
      "Q 892+346 T 1238 \u001b[92m☑\u001b[0m 1238\n",
      "Q 757+56  T 813  \u001b[92m☑\u001b[0m 813 \n",
      "Q 72+665  T 737  \u001b[92m☑\u001b[0m 737 \n",
      "Q 212+89  T 301  \u001b[92m☑\u001b[0m 301 \n",
      "Q 830+0   T 830  \u001b[92m☑\u001b[0m 830 \n",
      "Q 3+484   T 487  \u001b[92m☑\u001b[0m 487 \n",
      "Q 712+512 T 1224 \u001b[91m☒\u001b[0m 1234\n",
      "Q 79+499  T 578  \u001b[92m☑\u001b[0m 578 \n",
      "Q 834+68  T 902  \u001b[92m☑\u001b[0m 902 \n",
      "Q 482+25  T 507  \u001b[92m☑\u001b[0m 507 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 26\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 8s 188us/step - loss: 0.0394 - acc: 0.9916 - val_loss: 0.0216 - val_acc: 0.9978\n",
      "Q 98+919  T 1017 \u001b[92m☑\u001b[0m 1017\n",
      "Q 264+191 T 455  \u001b[92m☑\u001b[0m 455 \n",
      "Q 758+387 T 1145 \u001b[92m☑\u001b[0m 1145\n",
      "Q 192+78  T 270  \u001b[92m☑\u001b[0m 270 \n",
      "Q 48+193  T 241  \u001b[92m☑\u001b[0m 241 \n",
      "Q 189+459 T 648  \u001b[92m☑\u001b[0m 648 \n",
      "Q 20+319  T 339  \u001b[92m☑\u001b[0m 339 \n",
      "Q 87+1    T 88   \u001b[92m☑\u001b[0m 88  \n",
      "Q 327+576 T 903  \u001b[92m☑\u001b[0m 903 \n",
      "Q 93+397  T 490  \u001b[91m☒\u001b[0m 480 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 27\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 160us/step - loss: 0.0154 - acc: 0.9992 - val_loss: 0.0178 - val_acc: 0.9983\n",
      "Q 657+43  T 700  \u001b[92m☑\u001b[0m 700 \n",
      "Q 55+424  T 479  \u001b[92m☑\u001b[0m 479 \n",
      "Q 184+170 T 354  \u001b[92m☑\u001b[0m 354 \n",
      "Q 767+9   T 776  \u001b[92m☑\u001b[0m 776 \n",
      "Q 43+417  T 460  \u001b[92m☑\u001b[0m 460 \n",
      "Q 672+17  T 689  \u001b[92m☑\u001b[0m 689 \n",
      "Q 331+28  T 359  \u001b[92m☑\u001b[0m 359 \n",
      "Q 892+0   T 892  \u001b[92m☑\u001b[0m 892 \n",
      "Q 3+657   T 660  \u001b[92m☑\u001b[0m 660 \n",
      "Q 68+630  T 698  \u001b[92m☑\u001b[0m 698 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 28\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n",
      "45000/45000 [==============================] - 7s 163us/step - loss: 0.0270 - acc: 0.9949 - val_loss: 0.0223 - val_acc: 0.9963\n",
      "Q 687+35  T 722  \u001b[92m☑\u001b[0m 722 \n",
      "Q 19+797  T 816  \u001b[92m☑\u001b[0m 816 \n",
      "Q 881+47  T 928  \u001b[92m☑\u001b[0m 928 \n",
      "Q 470+3   T 473  \u001b[92m☑\u001b[0m 473 \n",
      "Q 330+34  T 364  \u001b[92m☑\u001b[0m 364 \n",
      "Q 992+77  T 1069 \u001b[92m☑\u001b[0m 1069\n",
      "Q 80+704  T 784  \u001b[92m☑\u001b[0m 784 \n",
      "Q 337+358 T 695  \u001b[92m☑\u001b[0m 695 \n",
      "Q 45+1    T 46   \u001b[92m☑\u001b[0m 46  \n",
      "Q 8+760   T 768  \u001b[92m☑\u001b[0m 768 \n",
      "\n",
      "--------------------------------------------------\n",
      "Iteration 29\n",
      "Train on 45000 samples, validate on 5000 samples\n",
      "Epoch 1/1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45000/45000 [==============================] - 7s 159us/step - loss: 0.0119 - acc: 0.9995 - val_loss: 0.0144 - val_acc: 0.9984\n",
      "Q 3+0     T 3    \u001b[91m☒\u001b[0m 4   \n",
      "Q 636+444 T 1080 \u001b[92m☑\u001b[0m 1080\n",
      "Q 951+36  T 987  \u001b[92m☑\u001b[0m 987 \n",
      "Q 650+39  T 689  \u001b[92m☑\u001b[0m 689 \n",
      "Q 71+42   T 113  \u001b[92m☑\u001b[0m 113 \n",
      "Q 178+29  T 207  \u001b[92m☑\u001b[0m 207 \n",
      "Q 618+7   T 625  \u001b[92m☑\u001b[0m 625 \n",
      "Q 12+678  T 690  \u001b[92m☑\u001b[0m 690 \n",
      "Q 810+499 T 1309 \u001b[92m☑\u001b[0m 1309\n",
      "Q 699+568 T 1267 \u001b[92m☑\u001b[0m 1267\n"
     ]
    }
   ],
   "source": [
    "for iteration in range(1, 30):\n",
    "    print()\n",
    "    print('-' * 50)\n",
    "    print('Iteration', iteration)\n",
    "    model.fit(x_train, y_train,\n",
    "             batch_size=BATCH_SIZE,\n",
    "             epochs=1,\n",
    "             validation_data=(x_val, y_val))\n",
    "    \n",
    "    for i in range(10):\n",
    "        ind = np.random.randint(0, len(x_val))\n",
    "        rowx, rowy = x_val[np.array([ind])], y_val[np.array([ind])]\n",
    "        preds = model.predict_classes(rowx, verbose=0)\n",
    "        \n",
    "        q = ctable.decode(rowx[0])\n",
    "        correct = ctable.decode(rowy[0])\n",
    "        guess = ctable.decode(preds[0], calc_argmax=False)\n",
    "        print('Q', q[::-1] if INVERT else q, end=' ')\n",
    "        print('T', correct, end=' ')\n",
    "        if correct == guess:\n",
    "            print(colors.ok + '☑' + colors.close, end=' ')\n",
    "        else:\n",
    "            print(colors.fail + '☒' + colors.close, end=' ')\n",
    "        print(guess)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A precondition for the above method is that it assumes that a given fixed-length sequence may yield a fixed-length target [... t] sequence when input [... t].\n",
    "\n",
    "This works in some situations, but not in most usage scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The general case: canonical sequence-to-sequence\n",
    "\n",
    "In the general case, input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. This requires a more advanced setup, which is what people commonly refer to when mentioning \"sequence to sequence models\" with no further context. Here's how it works:\n",
    "\n",
    "* A RNN layer (or stack thereof) acts as \"encoder\": it processes the input sequence and returns its own internal state. Note that we discard the outputs of the encoder RNN, only recovering the state. This state will serve as the \"context\", or \"conditioning\", of the decoder in the next step.\n",
    "\n",
    "\n",
    "* Another RNN layer (or stack thereof) acts as \"decoder\": it is trained to predict the next characters of the target sequence, given previous characters of the target sequence. Specifically, it is trained to turn the target sequences into the same sequences but offset by one timestep in the future, a training process called \"teacher forcing\" in this context. Importantly, the encoder uses as initial state the state vectors from the encoder, which is how the decoder obtains information about what it is supposed to generate. Effectively, the decoder learns to generate targets[t+1...] given targets[...t], *conditioned on the input sequence*.\n",
    "\n",
    "![lstm_seq](https://blog.keras.io/img/seq2seq/seq2seq-teacher-forcing.png)\n",
    "\n",
    "In inference mode, i.e. when we want to decode unknown input sequences, we go through a slightly different process:\n",
    "\n",
    "1. Encode the input sequence into state vectors.\n",
    "2. Start with a target sequence of size 1 (just the start-of-sequence character).\n",
    "3. Feed the state vectors and 1-char target sequence to the decoder to produce predictions for the next character.\n",
    "4. Sample the next character using these predictions (we simply use argmax).\n",
    "5. Append the sampled character to the target sequence\n",
    "6. Repeat until we generate the end-of-sequence character or we hit the character limit.\n",
    "\n",
    "![lstm_seq2](https://blog.keras.io/img/seq2seq/seq2seq-inference.png)\n",
    "\n",
    "The same process can also be used to train a Seq2Seq network without \"teacher forcing\", i.e. by reinjecting the decoder's predictions into the decoder.\n",
    "\n",
    "In order to achieve our example, we will use the English sentence corresponding to the Chinese translation of the data set\n",
    "\n",
    "We will implement a character-level sequence-to-sequence model, process the input character by character, and produce the output character by character. Another option is a word-level model, which is often more common with machine translation. At the end of this article, you'll find some reference links about converting our models to word-level models using embedding layers.\n",
    "\n",
    "## data set\n",
    "[download](https://github.com/erhwenkuo/deep-learning-with-keras-notebooks/blob/master/assets/data/cmn-tw.txt)\n",
    "\n",
    "Here's a summary of our process:\n",
    "\n",
    "1. Turn the sentences into 3 Numpy arrays, encoder_input_data, decoder_input_data, decoder_target_data:\n",
    "  * encoder_input_data is a 3D array of shape (num_pairs, max_english_sentence_length, num_english_characters) containing a one-hot vectorization of the English sentences.\n",
    "  * decoder_input_data is a 3D array of shape (num_pairs, max_chinese_sentence_length, num_chinese_characters) containg a one-hot vectorization of the Chinese sentences.\n",
    "  * decoder_target_data is the same as decoder_input_data but offset by one timestep. decoder_target_data[: t :] will be the same as decoder_input_data[: t + 1 :].\n",
    "  \n",
    "2. Train a basic LSTM-based Seq2Seq model to predict decoder_target_data given encoder_input_data and decoder_input_data. Our model uses **teacher forcing**.\n",
    "\n",
    "3. Decode some sentences to check that the model is working (i.e. turn samples from encoder_input_data into corresponding samples from decoder_target_data).\n",
    "\n",
    "![lstm](https://imgur.com/4jGulCj.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "\n",
    "DATA_FILE = \"cmn-tw.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 64 \n",
    "epochs = 100 \n",
    "latent_dim = 256 # Latent dimensionality of the encoding space.\n",
    "num_samples = 10000 # Number of samples to train on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 73\n",
      "Number of unique output tokens: 2165\n",
      "Max sequence length for inputs: 33\n",
      "Max sequence length for outputs: 22\n"
     ]
    }
   ],
   "source": [
    "# Vectorize the data.\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set() # English character set\n",
    "target_characters = set() # Chinese character set\n",
    "lines = open(DATA_FILE, mode=\"r\", encoding=\"utf-8\").read().split('\\n')\n",
    "\n",
    "# Read and process line by line\n",
    "for line in lines[: min(num_samples, len(lines)-1)]:\n",
    "    input_text, target_text = line.split('\\t')\n",
    "    \n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    \n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "      \n",
    "    \n",
    "input_characters = sorted(list(input_characters)) # All input character set\n",
    "target_characters = sorted(list(target_characters)) # All target character set\n",
    "\n",
    "num_encoder_tokens = len(input_characters) # The number of all input characters\n",
    "num_decoder_tokens = len(target_characters) # he number of all target characters\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts]) # The max sequence length for inputs\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts]) # The max sequence length for outputs\n",
    "\n",
    "print('Number of samples:', len(input_texts))\n",
    "print('Number of unique input tokens:', num_encoder_tokens)\n",
    "print('Number of unique output tokens:', num_decoder_tokens)\n",
    "print('Max sequence length for inputs:', max_encoder_seq_length)\n",
    "print('Max sequence length for outputs:', max_decoder_seq_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# the index dictionary of input tokens\n",
    "input_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(input_characters)])\n",
    "\n",
    "# the index dictionary of target tokens\n",
    "target_token_index = dict(\n",
    "    [(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "# One-hot vectorized three-dimensional shape array containing English sentences\n",
    "#（num_pairs，max_english_sentence_length，num_english_characters）\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# One-hot vectorized three-dimensional shape array containing Chinese sentences\n",
    "#（num_pairs，max_chinese_sentence_length，num_chinese_characters）\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "# decoder_target_data is the same as decoder_input_data, but offset by one time step.\n",
    "# decoder_target_data [: t :] will be the same as decoder_input_data [: t + 1 :]\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens),\n",
    "    dtype='float32')\n",
    "\n",
    "\n",
    "# Transform data into tensor data structures to be used for training \n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "        \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input (InputLayer)      (None, None, 73)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input (InputLayer)      (None, None, 2165)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 337920      encoder_input[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  2480128     decoder_input[0][0]              \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2165)   556405      decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,374,453\n",
      "Trainable params: 3,374,453\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# ===== encoder ====\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "# Note: Input_shape = (None, num_features) is used because the timesteps is variable.\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), name='encoder_input') \n",
    "# Need to get the internal state of LSTM, so set \"return_state = True\"\n",
    "encoder = LSTM(latent_dim, return_state=True, name='encoder_lstm') \n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "# ==== decoder ====\n",
    "\n",
    "# Set up the decoder\n",
    "# Note: Input_shape = (None, num_features) is used because the timesteps is variable.\n",
    "decoder_inputs = Input(shape=(None, num_decoder_tokens), name='decoder_input')\n",
    "\n",
    "\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. \n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True, name='decoder_lstm')\n",
    "\n",
    "# We don't use the return states in the training model, but we will use them in inference.\n",
    "# The initial state of the decoder is to use the last state of the encoder\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs,\n",
    "                                     initial_state=encoder_states) # we using `encoder_states` as initial state.\n",
    "\n",
    "\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_output')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAccAAAFgCAYAAADO5bLkAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "from IPython.display import Image\n",
    "\n",
    "plot_model(model, to_file='seq2seq_graph.png')\n",
    "Image('seq2seq_graph.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 8000 samples, validate on 2000 samples\n",
      "Epoch 1/100\n",
      "8000/8000 [==============================] - 22s 3ms/step - loss: 2.0068 - val_loss: 2.4574\n",
      "Epoch 2/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.8665 - val_loss: 2.3594\n",
      "Epoch 3/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.7417 - val_loss: 2.2756\n",
      "Epoch 4/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.6407 - val_loss: 2.1442\n",
      "Epoch 5/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.5532 - val_loss: 2.0655\n",
      "Epoch 6/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.4732 - val_loss: 1.9869\n",
      "Epoch 7/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.4020 - val_loss: 1.9881\n",
      "Epoch 8/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.3421 - val_loss: 1.8980\n",
      "Epoch 9/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.2865 - val_loss: 1.8544\n",
      "Epoch 10/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.2358 - val_loss: 1.8229\n",
      "Epoch 11/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.1922 - val_loss: 1.7979\n",
      "Epoch 12/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.1517 - val_loss: 1.7781\n",
      "Epoch 13/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.1163 - val_loss: 1.7578\n",
      "Epoch 14/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 1.0785 - val_loss: 1.7400\n",
      "Epoch 15/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.0452 - val_loss: 1.7321\n",
      "Epoch 16/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 1.0155 - val_loss: 1.7263\n",
      "Epoch 17/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.9861 - val_loss: 1.7161\n",
      "Epoch 18/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.9594 - val_loss: 1.7274\n",
      "Epoch 19/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.9339 - val_loss: 1.7031\n",
      "Epoch 20/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.9078 - val_loss: 1.7124\n",
      "Epoch 21/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.8844 - val_loss: 1.7035\n",
      "Epoch 22/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.8603 - val_loss: 1.7009\n",
      "Epoch 23/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.8372 - val_loss: 1.7093\n",
      "Epoch 24/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.8153 - val_loss: 1.7091\n",
      "Epoch 25/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7945 - val_loss: 1.7072\n",
      "Epoch 26/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.7737 - val_loss: 1.7113\n",
      "Epoch 27/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7538 - val_loss: 1.7200\n",
      "Epoch 28/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.7354 - val_loss: 1.7141\n",
      "Epoch 29/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.7157 - val_loss: 1.7297\n",
      "Epoch 30/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.6988 - val_loss: 1.7316\n",
      "Epoch 31/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6805 - val_loss: 1.7368\n",
      "Epoch 32/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.6628 - val_loss: 1.7375\n",
      "Epoch 33/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.6468 - val_loss: 1.7595\n",
      "Epoch 34/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.6306 - val_loss: 1.7508\n",
      "Epoch 35/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.6143 - val_loss: 1.7777\n",
      "Epoch 36/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5989 - val_loss: 1.7618\n",
      "Epoch 37/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.5833 - val_loss: 1.7707\n",
      "Epoch 38/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5698 - val_loss: 1.7779\n",
      "Epoch 39/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5552 - val_loss: 1.7776\n",
      "Epoch 40/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5412 - val_loss: 1.7872\n",
      "Epoch 41/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5287 - val_loss: 1.7817\n",
      "Epoch 42/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5147 - val_loss: 1.7993\n",
      "Epoch 43/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.5024 - val_loss: 1.8086\n",
      "Epoch 44/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4907 - val_loss: 1.8128\n",
      "Epoch 45/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4790 - val_loss: 1.8253\n",
      "Epoch 46/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4669 - val_loss: 1.8195\n",
      "Epoch 47/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4559 - val_loss: 1.8321\n",
      "Epoch 48/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.4457 - val_loss: 1.8440\n",
      "Epoch 49/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.4349 - val_loss: 1.8595\n",
      "Epoch 50/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4249 - val_loss: 1.8520\n",
      "Epoch 51/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.4150 - val_loss: 1.8721\n",
      "Epoch 52/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.4049 - val_loss: 1.8718\n",
      "Epoch 53/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3958 - val_loss: 1.8749\n",
      "Epoch 54/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3869 - val_loss: 1.8791\n",
      "Epoch 55/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3784 - val_loss: 1.8943\n",
      "Epoch 56/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3692 - val_loss: 1.9074\n",
      "Epoch 57/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3612 - val_loss: 1.8991\n",
      "Epoch 58/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3533 - val_loss: 1.9076\n",
      "Epoch 59/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.3453 - val_loss: 1.9212\n",
      "Epoch 60/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3372 - val_loss: 1.9306\n",
      "Epoch 61/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.3299 - val_loss: 1.9343\n",
      "Epoch 62/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.3221 - val_loss: 1.9386\n",
      "Epoch 63/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.3149 - val_loss: 1.9440\n",
      "Epoch 64/100\n",
      "8000/8000 [==============================] - 20s 3ms/step - loss: 0.3077 - val_loss: 1.9530\n",
      "Epoch 65/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.3013 - val_loss: 1.9724\n",
      "Epoch 66/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2941 - val_loss: 1.9666\n",
      "Epoch 67/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2875 - val_loss: 1.9683\n",
      "Epoch 68/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2817 - val_loss: 1.9815\n",
      "Epoch 69/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2751 - val_loss: 2.0023\n",
      "Epoch 70/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2685 - val_loss: 1.9996\n",
      "Epoch 71/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2630 - val_loss: 2.0074\n",
      "Epoch 72/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2565 - val_loss: 2.0129\n",
      "Epoch 73/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.2510 - val_loss: 2.0221\n",
      "Epoch 74/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.2459 - val_loss: 2.0264\n",
      "Epoch 75/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2400 - val_loss: 2.0303\n",
      "Epoch 76/100\n",
      "8000/8000 [==============================] - 20s 2ms/step - loss: 0.2342 - val_loss: 2.0410\n",
      "Epoch 77/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2291 - val_loss: 2.0402\n",
      "Epoch 78/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2242 - val_loss: 2.0477\n",
      "Epoch 79/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2190 - val_loss: 2.0543\n",
      "Epoch 80/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2141 - val_loss: 2.0631\n",
      "Epoch 81/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2094 - val_loss: 2.0632\n",
      "Epoch 82/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.2048 - val_loss: 2.0725\n",
      "Epoch 83/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.2001 - val_loss: 2.0723\n",
      "Epoch 84/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1951 - val_loss: 2.0887\n",
      "Epoch 85/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1908 - val_loss: 2.0891\n",
      "Epoch 86/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1866 - val_loss: 2.1036\n",
      "Epoch 87/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1817 - val_loss: 2.1082\n",
      "Epoch 88/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1773 - val_loss: 2.1102\n",
      "Epoch 89/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1740 - val_loss: 2.1208\n",
      "Epoch 90/100\n",
      "8000/8000 [==============================] - 17s 2ms/step - loss: 0.1695 - val_loss: 2.1262\n",
      "Epoch 91/100\n",
      "8000/8000 [==============================] - 21s 3ms/step - loss: 0.1655 - val_loss: 2.1295\n",
      "Epoch 92/100\n",
      "8000/8000 [==============================] - 23s 3ms/step - loss: 0.1611 - val_loss: 2.1389\n",
      "Epoch 93/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1573 - val_loss: 2.1482\n",
      "Epoch 94/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1532 - val_loss: 2.1565\n",
      "Epoch 95/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1496 - val_loss: 2.1601\n",
      "Epoch 96/100\n",
      "8000/8000 [==============================] - 18s 2ms/step - loss: 0.1459 - val_loss: 2.1611\n",
      "Epoch 97/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1419 - val_loss: 2.1712\n",
      "Epoch 98/100\n",
      "8000/8000 [==============================] - 20s 3ms/step - loss: 0.1387 - val_loss: 2.1765\n",
      "Epoch 99/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1354 - val_loss: 2.1906\n",
      "Epoch 100/100\n",
      "8000/8000 [==============================] - 19s 2ms/step - loss: 0.1321 - val_loss: 2.1893\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit([encoder_input_data, decoder_input_data], decoder_target_data,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\keras\\engine\\topology.py:2344: UserWarning: Layer decoder_lstm was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'encoder_lstm/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'encoder_lstm/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  str(node.arguments) + '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def show_train_history(train_history, train, validation):  \n",
    "    plt.plot(train_history.history[train])  \n",
    "    plt.plot(train_history.history[validation])  \n",
    "    plt.title('Train History')  \n",
    "    plt.ylabel(train)  \n",
    "    plt.xlabel('Epoch')  \n",
    "    plt.legend(['train', 'validation'], loc='upper left')  \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEWCAYAAACJ0YulAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRysAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x17fa6c433c8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "show_train_history(history, 'loss', 'val_loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model prediction\n",
    "\n",
    "## Here's the drill:\n",
    "1. Encode the input sentence and retrieve the initial decoder state\n",
    "2. Run one step of the decoder with this initial state and a \"start of sequence\" token as target. The output will be the next target character.\n",
    "3. Append the target character predicted and repeat.\n",
    "\n",
    "![ex](https://imgur.com/b5z7vt0.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "\n",
    "# Define the model of the encoder\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Define initial weight input for decoder LSTM cell\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "\n",
    "# Define the initial decoder state of the decoder\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs) # We use `decoder_states_inputs` as the initial state\n",
    "\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model of the decoder\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs] + decoder_states)\n",
    "\n",
    "\n",
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict(\n",
    "    (i, char) for char, i in input_token_index.items())\n",
    "\n",
    "reverse_target_char_index = dict(\n",
    "    (i, char) for char, i in target_token_index.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Decode the sequence\n",
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "    \n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    \n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    # In this example, the \"\\ t\" character is used\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        \n",
    "        # Exit condition: either hit max length or find stop characte\n",
    "        # In this example, the \"\\n\" character is used\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Hi.\n",
      "Decoded sentence: 嗨。\n",
      "\n",
      "-\n",
      "Input sentence: Run.\n",
      "Decoded sentence: 你用跑的。\n",
      "\n",
      "-\n",
      "Input sentence: Wait!\n",
      "Decoded sentence: 等等！\n",
      "\n",
      "-\n",
      "Input sentence: Hello!\n",
      "Decoded sentence: 你好。\n",
      "\n",
      "-\n",
      "Input sentence: I try.\n",
      "Decoded sentence: 我信任湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: I won!\n",
      "Decoded sentence: 我不會做的天到。\n",
      "\n",
      "-\n",
      "Input sentence: Oh no!\n",
      "Decoded sentence: 不會吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cheers!\n",
      "Decoded sentence: 乾杯!\n",
      "\n",
      "-\n",
      "Input sentence: He ran.\n",
      "Decoded sentence: 他跑了。\n",
      "\n",
      "-\n",
      "Input sentence: Hop in.\n",
      "Decoded sentence: 停了。\n",
      "\n",
      "-\n",
      "Input sentence: I lost.\n",
      "Decoded sentence: 我喜歡閱讀。\n",
      "\n",
      "-\n",
      "Input sentence: I quit.\n",
      "Decoded sentence: 我喜歡車。\n",
      "\n",
      "-\n",
      "Input sentence: I'm OK.\n",
      "Decoded sentence: 我沒事。\n",
      "\n",
      "-\n",
      "Input sentence: Listen.\n",
      "Decoded sentence: 聽著。\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒門！\n",
      "\n",
      "-\n",
      "Input sentence: No way!\n",
      "Decoded sentence: 沒門！\n",
      "\n",
      "-\n",
      "Input sentence: Really?\n",
      "Decoded sentence: 你確定？\n",
      "\n",
      "-\n",
      "Input sentence: Try it.\n",
      "Decoded sentence: 試試吧。\n",
      "\n",
      "-\n",
      "Input sentence: We try.\n",
      "Decoded sentence: 我們來試試。\n",
      "\n",
      "-\n",
      "Input sentence: Why me?\n",
      "Decoded sentence: 為什麼是我？\n",
      "\n",
      "-\n",
      "Input sentence: Ask Tom.\n",
      "Decoded sentence: 去問湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Be calm.\n",
      "Decoded sentence: 把它弄小一點。\n",
      "\n",
      "-\n",
      "Input sentence: Be fair.\n",
      "Decoded sentence: 小心狗！\n",
      "\n",
      "-\n",
      "Input sentence: Be kind.\n",
      "Decoded sentence: 友善點。\n",
      "\n",
      "-\n",
      "Input sentence: Be nice.\n",
      "Decoded sentence: 在這裡等著。\n",
      "\n",
      "-\n",
      "Input sentence: Call me.\n",
      "Decoded sentence: 打掃我們房間。\n",
      "\n",
      "-\n",
      "Input sentence: Call us.\n",
      "Decoded sentence: 打掃房間。\n",
      "\n",
      "-\n",
      "Input sentence: Come in.\n",
      "Decoded sentence: 跟我來。\n",
      "\n",
      "-\n",
      "Input sentence: Get Tom.\n",
      "Decoded sentence: 來自湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Get out!\n",
      "Decoded sentence: 滾出去！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Go away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Go away.\n",
      "Decoded sentence: 進來。\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Goodbye!\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Hang on!\n",
      "Decoded sentence: 等一下！\n",
      "\n",
      "-\n",
      "Input sentence: He came.\n",
      "Decoded sentence: 他來了。\n",
      "\n",
      "-\n",
      "Input sentence: He runs.\n",
      "Decoded sentence: 他跑。\n",
      "\n",
      "-\n",
      "Input sentence: Help me.\n",
      "Decoded sentence: 幫我一下。\n",
      "\n",
      "-\n",
      "Input sentence: Hold on.\n",
      "Decoded sentence: 嘿，你是走。\n",
      "\n",
      "-\n",
      "Input sentence: Hug Tom.\n",
      "Decoded sentence: 趕快回家。\n",
      "\n",
      "-\n",
      "Input sentence: I agree.\n",
      "Decoded sentence: 我同意。\n",
      "\n",
      "-\n",
      "Input sentence: I'm ill.\n",
      "Decoded sentence: 我會兒。\n",
      "\n",
      "-\n",
      "Input sentence: I'm old.\n",
      "Decoded sentence: 我是清白的。\n",
      "\n",
      "-\n",
      "Input sentence: It's OK.\n",
      "Decoded sentence: 沒關係。\n",
      "\n",
      "-\n",
      "Input sentence: It's me.\n",
      "Decoded sentence: 是我。\n",
      "\n",
      "-\n",
      "Input sentence: Join us.\n",
      "Decoded sentence: 來加入我們吧。\n",
      "\n",
      "-\n",
      "Input sentence: Keep it.\n",
      "Decoded sentence: 繼續看。\n",
      "\n",
      "-\n",
      "Input sentence: Kiss me.\n",
      "Decoded sentence: 在這裡著著。\n",
      "\n",
      "-\n",
      "Input sentence: Perfect!\n",
      "Decoded sentence: 把它給你。\n",
      "\n",
      "-\n",
      "Input sentence: See you.\n",
      "Decoded sentence: 再見！\n",
      "\n",
      "-\n",
      "Input sentence: Shut up!\n",
      "Decoded sentence: 閉嘴！\n",
      "\n",
      "-\n",
      "Input sentence: Skip it.\n",
      "Decoded sentence: 不管它。\n",
      "\n",
      "-\n",
      "Input sentence: Take it.\n",
      "Decoded sentence: 拿走吧。\n",
      "\n",
      "-\n",
      "Input sentence: Wake up!\n",
      "Decoded sentence: 問題！\n",
      "\n",
      "-\n",
      "Input sentence: Wash up.\n",
      "Decoded sentence: 去清洗一下。\n",
      "\n",
      "-\n",
      "Input sentence: We know.\n",
      "Decoded sentence: 我們知道。\n",
      "\n",
      "-\n",
      "Input sentence: Welcome.\n",
      "Decoded sentence: 歡迎。\n",
      "\n",
      "-\n",
      "Input sentence: Who won?\n",
      "Decoded sentence: 誰贏了？\n",
      "\n",
      "-\n",
      "Input sentence: Why not?\n",
      "Decoded sentence: 為什麼不？\n",
      "\n",
      "-\n",
      "Input sentence: You run.\n",
      "Decoded sentence: 你跑。\n",
      "\n",
      "-\n",
      "Input sentence: Back off.\n",
      "Decoded sentence: 往後退點。\n",
      "\n",
      "-\n",
      "Input sentence: Be still.\n",
      "Decoded sentence: 讓我們開始吧。\n",
      "\n",
      "-\n",
      "Input sentence: Cuff him.\n",
      "Decoded sentence: 把他銬上。\n",
      "\n",
      "-\n",
      "Input sentence: Drive on.\n",
      "Decoded sentence: 往前開。\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get away!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get down!\n",
      "Decoded sentence: 趴下！\n",
      "\n",
      "-\n",
      "Input sentence: Get lost!\n",
      "Decoded sentence: 滾！\n",
      "\n",
      "-\n",
      "Input sentence: Get real.\n",
      "Decoded sentence: 讓他們。\n",
      "\n",
      "-\n",
      "Input sentence: Grab Tom.\n",
      "Decoded sentence: 抓住湯姆。\n",
      "\n",
      "-\n",
      "Input sentence: Grab him.\n",
      "Decoded sentence: 抓住他。\n",
      "\n",
      "-\n",
      "Input sentence: Have fun.\n",
      "Decoded sentence: 玩得開心。\n",
      "\n",
      "-\n",
      "Input sentence: He tries.\n",
      "Decoded sentence: 他來試試。\n",
      "\n",
      "-\n",
      "Input sentence: Humor me.\n",
      "Decoded sentence: 你就隨了我的意吧。\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: Hurry up.\n",
      "Decoded sentence: 快點！\n",
      "\n",
      "-\n",
      "Input sentence: I forgot.\n",
      "Decoded sentence: 我忘了。\n",
      "\n",
      "-\n",
      "Input sentence: I resign.\n",
      "Decoded sentence: 我在這裡。\n",
      "\n",
      "-\n",
      "Input sentence: I'll pay.\n",
      "Decoded sentence: 我會開車。\n",
      "\n",
      "-\n",
      "Input sentence: I'm busy.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm cold.\n",
      "Decoded sentence: 我是從中國。\n",
      "\n",
      "-\n",
      "Input sentence: I'm fine.\n",
      "Decoded sentence: 我是一個人。\n",
      "\n",
      "-\n",
      "Input sentence: I'm full.\n",
      "Decoded sentence: 我很好。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: I'm sick.\n",
      "Decoded sentence: 我生病了。\n",
      "\n",
      "-\n",
      "Input sentence: Leave me.\n",
      "Decoded sentence: 讓我一個人呆會兒。\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Let's go!\n",
      "Decoded sentence: 我們走吧!\n",
      "\n",
      "-\n",
      "Input sentence: Look out!\n",
      "Decoded sentence: 看看！\n",
      "\n",
      "-\n",
      "Input sentence: She runs.\n",
      "Decoded sentence: 她跑。\n",
      "\n",
      "-\n",
      "Input sentence: Stand up.\n",
      "Decoded sentence: 祝你好。\n",
      "\n",
      "-\n",
      "Input sentence: They won.\n",
      "Decoded sentence: 他們贏了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom died.\n",
      "Decoded sentence: 湯姆去世了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom quit.\n",
      "Decoded sentence: 湯姆不干了。\n",
      "\n",
      "-\n",
      "Input sentence: Tom swam.\n",
      "Decoded sentence: 湯姆游泳了。\n",
      "\n",
      "-\n",
      "Input sentence: Trust me.\n",
      "Decoded sentence: 相信我。\n",
      "\n",
      "-\n",
      "Input sentence: Try hard.\n",
      "Decoded sentence: 努力。\n",
      "\n",
      "Wall time: 3.02 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for seq_index in range(100):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', input_texts[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference:\n",
    "\n",
    "[sequence-to-sequence learning in Keras](https://github.com/keras-team/keras/blob/master/examples/lstm_seq2seq.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_input (InputLayer)   (None, None, 73)          0         \n",
      "_________________________________________________________________\n",
      "encoder_lstm (LSTM)          [(None, 256), (None, 256) 337920    \n",
      "=================================================================\n",
      "Total params: 337,920\n",
      "Trainable params: 337,920\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOUAAACdCAYAAABLuL+6AAAABmJLR0QA/wD/AP+gvaeTAAAOEUlE\nQVR4nO3dT2gj5R8G8Ge2rjep+GcFxYIiuxex17oIxUUPKhM92GWT7m7xsDI5CntQmLBCPaa3Qkt6\nkqWbsEWEBPTUHgpuCiJkD4LZw8JUFpyckosg7Pr9Her7/mYmk3SSZpI32+cDYZs378z7nck8ycy7\naWqJiICIjHFm0gUQURhDSWQYhpLIMAwlkWGeiTb89ddf+Oqrr/DkyZNJ1EN0qly7dg22bYfaut4p\n9/b2UKlUxlYU0Wm1s7MTm7Wud0rl7t27qRZEdNotLy/HtvOaksgwDCWRYRhKIsMwlESGYSiJDMNQ\nEhmGoSQyDENJZBiGksgwDCWRYRhKIsMwlESGYSiJDMNQEhnm1ISy1WqhUqkgk8mMbcxCoYBCoTC2\n8ejp0PP3KZ82t27dwubm5qTLGKtOp4Pnn38eg3yLqGVZse2T+CbSaP0m1ZamU/NOubGxMfYxV1dX\nsbq6OvZxlf39/YGXERG02219v91uT+ygj9YvIvB9X9+fZG1pOjWhPG06nQ62traGWnZ2djb253Hq\nVf+5c+f0z5OqLW0jC2Wr1cLa2hosy0Imk8He3p5uD17L1Wo13efw8DC0jk6ng0qlAsuyYFlW7JMS\n16fVavXtl8lk8ODBg4HrrtVqyGQy6HQ6yOfzA10fRrc7yX4IjgkAW1tbsCwL+Xw+VL/a9uDpXLSt\nWCyiVquFHgOGv841pf5BqGCr5QuFQuj5Vre1tTW9TPCx4HalcYz0JBHb29sS09yX7/ti27aUy2UR\nEdnd3RUA0mg0xLZtASAApF6vi4iI53kCQBzHCa3Htm1xXVffdxwndF/1KZVKoXFt25Z2u93Vz3Ec\n3V4ul3Udw9TdaDS66u0nuHz0fq/9oB4P9mm32+I4jgCQZrOp645ui1pXsC16X0TEdd2ufRonuqwp\n9fdrj1Lj+r7fVWu9Xo89BtW2+r6va03rGMnlcpLL5bq3L9owTCjVAR9aMaCf/LidGG1T61A7Q+Ro\nx9m2re+rHRLtA0DvNBGRarUaOghEjg6OXmMeV3c08EklOciS9Gk0GgJAisXiidc1bO0m1Z90u1zX\nDYUkulyxWBQA4nleqNbgsZTmMZJqKIOvGNFbsPDQwD1eiftRr3xBKmzB8Mb16zfmIHUPYlShHPW6\nhqndpPoH3S7P83QAg8upFwt15iVyFNRgSNM8RlIN5XGFjeoAOskTPsyYDOX0h7JUKolt29JsNmOX\nUy/g7XZbn2oPMlYaoRzp7GuvyZQk1LdE379//9g+cRM7juMMPfZJ6h6nk2yjCcZVfz6fBwBUKhV8\n+eWXWF9fx/nz5/vW9PPPP2N/fx8rKyux/cZ5jIwklKVSCQBw+/ZtdDodAP+fsUpKBW5zc1Ov4/Dw\nUO9gAMjlcgCAhw8f6jbVd2lpqauefgEfVd3joA6Ijz/+eMKVDGec9R8cHGBxcREAkM1mAQBzc3M9\n+8/Pz8NxHGSzWWxtbWFhYSH0+ESOkehb57Czr4g55/Y8L/SYuhgOTrpEZ7mCyzuO0zVZo2Zb1XLl\ncrnrlEPNtNm2ra8P1CSRWu8gdQ8juLzv+4n3g7qvJhva7ba4rhu6ZhaRrhlNNeEV3D61P33f15Ms\nSWZfg3WpWk2pv9/zotbRaDRCy3ueFzp9DU4UBpcLXlsqaR4jqV5TihwFwXVdvVNVGKIb06tN5GgH\nqHW4rhsKZLBPqVQKPflxM1+e5+kn3nGc0NR28ElJUnf0gEoi7olMsh/Uz8Ep91Kp1LWNnufpx6vV\nqohI1/apiQzXdXXbcaE8ru5J1p+0NjVWdHk1GxucyFHUdWectI6RXqG0/lu5dufOHSwvLz+VH1+a\nBuo/yad1/09j/Z1OB19//fXYP4qp/pbI9vZ2qJ0fs6NT7+7du6E5iUljKA0SnFWOm2E23TTVXygU\nQh+nu3Tp0qRL0k7Nr26NStLPYA5z+vbKK6+Efp6mU0BguupXM7KlUgk3btyYcDVhDOWA0jzQTD6I\nk5im+m/cuGFcGBWevhIZhqEkMgxDSWQYhpLIMAwlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQY\nhpLIMAwlkWF6/pbI5cuXx1kH0amzs7OjvwwuqOud8tKlS7hy5cpYiqLR2d/fN/4XiylsaWkpNmtd\n39FD08myLGxvb8e+8tJ04TUlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLIMAwlkWEYSiLD\nMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLIMAwlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLI\nMAwlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLIMAwlkWH4l5yn0A8//IBvvvkGr776qm77\n5ZdfcOHCBbz00ksAgHa7jffeew/r6+uTKpOGxFBOoUKhgO+++y5RXz6904enr1Mom80e2+fs2bP4\n9ttv0y+GRo7vlFPq7bffxu+//963zx9//IELFy6MqSIaFb5TTqmrV6/i7NmzsY9ZloV33nmHgZxS\nDOWUymazePz4cexjMzMzWFlZGXNFNCo8fZ1iCwsL+PXXX/Hvv/+G2i3Lwp9//onXXnttQpXRSfCd\ncoqtrKzAsqxQ25kzZ3Dx4kUGcooxlFPs888/72qzLAvXr1+fQDU0KgzlFHv55Zfx/vvvY2ZmRrdZ\nlhUbVpoeDOWUu379uv6AwMzMDD788EO88MILE66KToKhnHKfffaZ/q8REcHVq1cnXBGdFEM55Z57\n7jl88sknAIBnn30Wn3766YQropN6Jq0VP378GNVqFU+ePElrCPrPm2++qf/96aefJlzN6bCwsIDX\nX389nZVLSn788UcBwBtvT+Xtiy++SCs6kto75d9//w0A/C0FeuosLy/jn3/+SW39vKYkMgxDSWQY\nhpLIMAwlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLIMAwlkWEYSiLDMJREhmEoB9RqtVCp\nVJDJZJ7K8WjyGMoB3bp1C9lsFrVazdjxOp1O1/fBpsWyrNhbPwcHB8jn87AsC/l8Hnt7e10191pv\n0tvBwUHf8Qepd9wYygFtbGwYP97+/n4KlcQTEfi+r++32+2+v9h+cHCAd999F4uLixARbGxs4MUX\nX8S1a9e6+pbLZYiIvgXHVLdyuazbPM/Tfb7//vueNQQf833fuF/EZyifMp1OB1tbW2Md89y5c/rn\n2dnZvn1VIK5cuaLb5ufnsbq62tU32KeXjz76SP88NzcHACgWi9jc3MTh4WFX/8PDQ7z11luxtZvC\nuFC2Wi2sra3BsixkMhns7e3p9uC1Va1W032iO7/T6aBSqehTk7iDNK5Pq9Xq2y+TyeDBgwcD112r\n1ZDJZNDpdJDP51EoFE60jwDosVTd6hSsWCzqU121bb32XT6f1/tObWOwDTj6A7WjqFd59OgRAOD+\n/fuh9vn5+dD94LteP7Ozs119P/jgAwDAvXv3uvrfu3dPP26stL78Z3t7WwZdve/7Ytu2lMtlERHZ\n3d0VANJoNMS2bf2lRfV6XUREPM8TAOI4Tmg9tm2L67r6vuM4ofuqT6lUCo1r27a02+2ufo7j6PZy\nuazrGKbuRqPRVe9xouMVi0XxPE9ERNrttriuG3o82j9YQ6PREBGRer2u912//em6bte+S1JjL41G\nQ/ctlUpd+/ukY6jHHceJ7au2LWm9cXK5nORyuaGWTcKoUKoDPgiAPijidmS0Ta3D933dVq/XxbZt\nfV+FJtoHgA6WiEi1WhUA0mw2dVu73e455nF1D3IA9tvGaO2+7/cN5Unbhqmxn2azqUOj9nmSfTNI\nKNVzrF5wRI5eEHZ3dweuN+pUhTL4ih69iSQ7iNQ6+ol7FVVhC4a316ttv3eipHUPIrq8qqvXwWx6\nKJV6vR4KZ7VaPfEY0Ren6Lv+SepVTlUoj9tRSQ6ipE9ckrCdtN8gNQ1Sb7PZDL0QFIvFY8czMZSK\nOpM5LpiDhlKdwXieJ77vh86CTA6lcRM9AHpOpiRh2zaA7omEuD5xEzuO4ww99knqHsT58+dRrVbR\naDTgOA5u3ryJtbW1sYw9rHw+D+Bo8qnT6YQeW1hYwPr6OgCM9EMSFy9eBHA0ubO3t6fvm86oUJZK\nJQDA7du39ROnZjWTUoHb3NzU6zg8PNQHBQDkcjkAwMOHD3Wb6ru0tNRVT7+Aj6ruQagDe35+Hhsb\nG2g0Grh582YqY43CwcEBFhcX9f3ffvutq4/67wz1/I3C3NwcXNdFNpvFo0eP9BjGS+steNjZV8Rc\nl6nTD3VfXUcFJ13UxIeaCQ0u7zhO12SNmm1Vy5XL5a5ZUTUbadu2nu1UEwhqvYPUPYzg8qpW4GgS\nSdXkeV7oFFZtv+/7UiwWY/dd3Hrj2pLMvvbbRjWBpmZ9Vb/d3d3Q86hONVW/JPuhV5/g42q2N7je\nJOvq51RdU4ocHWBqit9xHH3gRQ/4Xm0iRztdrcN13VAgg31KpZJettekied5ejLCcZzQf38En9Ak\ndQcnkZLqtd0qcIi5plQHouu6sS8Yg+zP40IZ92IUd1P7Vq232WyG9n+v56nfGMf1UYIvtknWdZy0\nQ2mJpPMZozt37mB5edm4jzARndTy8jIAYHt7O5X1G3VNSUQMJZFxUvtTeNRf0l8X4un/6cNQTgjD\nRr3w9JXIMAwlkWEYSiLDMJREhmEoiQzDUBIZhqEkMgxDSWQYhpLIMAwlkWEYSiLDMJREhmEoiQyT\n+m+J7OzspD0E0Vjt7OyEvmBt1FILpfojKpcvX05rCKKJeeONN1Jbd2rf0UNEw+E1JZFhGEoiwzCU\nRIZhKIkM8z8Mg15DaVRkggAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(encoder_model, to_file='encoder_model.png')\n",
    "encoder_model.summary()\n",
    "Image('encoder_model.png')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input (InputLayer)      (None, None, 2165)   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  2480128     decoder_input[0][0]              \n",
      "                                                                 input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_output (Dense)          (None, None, 2165)   556405      decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,036,533\n",
      "Trainable params: 3,036,533\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmcAAAD/CAYAAAC0GHEWAAAABmJLR0QA/wD/AP+gvaeTAAAgAElE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\ngPP+D4mJKyZYi4PLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_model(decoder_model, to_file='decoder_model.png')\n",
    "decoder_model.summary()\n",
    "Image('decoder_model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_items([(' ', 0), ('!', 1), ('\"', 2), ('$', 3), ('%', 4), (\"'\", 5), (',', 6), ('-', 7), ('.', 8), ('0', 9), ('1', 10), ('2', 11), ('3', 12), ('4', 13), ('5', 14), ('6', 15), ('7', 16), ('8', 17), ('9', 18), (':', 19), ('?', 20), ('A', 21), ('B', 22), ('C', 23), ('D', 24), ('E', 25), ('F', 26), ('G', 27), ('H', 28), ('I', 29), ('J', 30), ('K', 31), ('L', 32), ('M', 33), ('N', 34), ('O', 35), ('P', 36), ('Q', 37), ('R', 38), ('S', 39), ('T', 40), ('U', 41), ('V', 42), ('W', 43), ('Y', 44), ('Z', 45), ('a', 46), ('b', 47), ('c', 48), ('d', 49), ('e', 50), ('f', 51), ('g', 52), ('h', 53), ('i', 54), ('j', 55), ('k', 56), ('l', 57), ('m', 58), ('n', 59), ('o', 60), ('p', 61), ('q', 62), ('r', 63), ('s', 64), ('t', 65), ('u', 66), ('v', 67), ('w', 68), ('x', 69), ('y', 70), ('z', 71), ('’', 72)])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
