{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Develop a Caption Generation Model\n",
    "Caption generation is a challenging artificial intelligence problem where a textual description must be generated for a photograph.\n",
    "\n",
    "It requires both methods from computer vision to understand the content of the image and a language model from the field of natural language processing to turn the understanding of the image into words in the right order. Recently, deep learning methods have achieved state of the art results on examples of this problem.\n",
    "\n",
    "It can be hard to develop caption generating models on your own data, primarily because the datasets and the models are so large and take days to train. An alternative approach is to explore model configurations with a small sample of the fuller dataset.\n",
    "\n",
    "![pic](https://imgur.com/JcN1RM4.png)\n",
    "\n",
    "The Caption Generation Model has 6 parts, they are:\n",
    "\n",
    "1. Data Preparation\n",
    "2. Baseline Caption Generation Model\n",
    "3. Network Size Parameters\n",
    "4. Configuring the Feature Extraction Model\n",
    "5. Word Embedding Models\n",
    "6. Analysis of Results\n",
    "\n",
    "# Data Preparation\n",
    "\n",
    "We will use the Flickr8K dataset that is comprised of a little more than 8,000 photographs and their descriptions.\n",
    "\n",
    "You can download the dataset from here:\n",
    "\n",
    "[Framing image description as a ranking task: data, models and evaluation metrics.](http://nlp.cs.illinois.edu/HockenmaierGroup/Framing_Image_Description/KCCA.html)\n",
    "\n",
    "Unzip the photographs and descriptions into your current working directory into Flicker8k_Dataset and Flickr8k_text directories respectively.\n",
    "\n",
    "There are two parts to the data preparation, they are:\n",
    "\n",
    "* Preparing the Text\n",
    "* Preparing the Photos\n",
    "\n",
    "## Preparing the Text\n",
    "\n",
    "The dataset contains multiple descriptions for each photograph and the text of the descriptions requires some minimal cleaning.\n",
    "\n",
    "First, we will load the file containing all of the descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    " \n",
    "filename = 'D:/Program/dataset/Flickr8K/Flickr8k_text/Flickr8k.token.txt'\n",
    "# load descriptions\n",
    "doc = load_doc(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each photo has a unique identifier. This is used in the photo filename and in the text file of descriptions. Next, we will step through the list of photo descriptions and save the first description for each photo. Below defines a function named `load_descriptions()` that, given the loaded document text, will return a dictionary of photo identifiers to descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: 8092 \n"
     ]
    }
   ],
   "source": [
    "# extract descriptions for images\n",
    "def load_descriptions(doc):\n",
    "    mapping = dict()\n",
    "    # process lines\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        if len(line) < 2:\n",
    "            continue\n",
    "        # take the first token as the image id, the rest as the description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # remove filename from image id\n",
    "        image_id = image_id.split('.')[0]\n",
    "        # convert description tokens back to string\n",
    "        image_desc = ' '.join(image_desc)\n",
    "        # store the first description for each image\n",
    "        if image_id not in mapping:\n",
    "            mapping[image_id] = image_desc\n",
    "    return mapping\n",
    " \n",
    "# parse descriptions\n",
    "descriptions = load_descriptions(doc)\n",
    "print('Loaded: {} '.format(len(descriptions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Next, we need to clean the description text.\n",
    "\n",
    "The descriptions are already tokenized and easy to work with. We will clean the text in the following ways in order to reduce the size of the vocabulary of words we will need to work with:\n",
    "\n",
    "* Convert all words to lowercase.\n",
    "* Remove all punctuation.\n",
    "* Remove all words that are one character or less in length (e.g. ‘a’).\n",
    "\n",
    "Below defines the `clean_descriptions()` function that, given the dictionary of image identifiers to descriptions, steps through each description and cleans the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Size: 4484\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def clean_descriptions(descriptions):\n",
    "    \"\"\"\n",
    "    This uses the 3-argument version of str.maketrans\n",
    "    with arguments (x, y, z) where 'x' and 'y'\n",
    "    must be equal-length strings and characters in 'x'\n",
    "    are replaced by characters in 'y'. 'z'\n",
    "    is a string (string.punctuation here)\n",
    "    where each character in the string is mapped\n",
    "    to None.\n",
    "    \n",
    "    This is an alternative that creates a dictionary mapping\n",
    "    of every character from string.punctuation to None (this will\n",
    "    also work)\n",
    "    #translator = str.maketrans(dict.fromkeys(string.punctuation))\n",
    "    \"\"\"\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for key, desc in descriptions.items():\n",
    "        # tokenize\n",
    "        desc = desc.split()\n",
    "        # convert to lower case\n",
    "        desc = [word.lower() for word in desc]\n",
    "        # remove punctuation from each token\n",
    "        desc = [w.translate(table) for w in desc]\n",
    "        # remove hanging 's' and 'a'\n",
    "        desc = [word for word in desc if len(word)>1]\n",
    "        # store as string\n",
    "        descriptions[key] =  ' '.join(desc)\n",
    "\n",
    "        \n",
    "# clean descriptions\n",
    "clean_descriptions(descriptions)\n",
    "# summarize vocabulary\n",
    "all_tokens = ' '.join(descriptions.values()).split()\n",
    "vocabulary = set(all_tokens)\n",
    "print('Vocabulary Size: {}'.format(len(vocabulary)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Finally, we save the dictionary of image identifiers and descriptions to a new file named *descriptions.txt*, with one image identifier and description per line.\n",
    "\n",
    "Below defines the `save_doc()` function that given a dictionary containing the mapping of identifiers to descriptions and a filename, saves the mapping to file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "def save_doc(descriptions, filename):\n",
    "    lines = list()\n",
    "    for key, desc in descriptions.items():\n",
    "        lines.append(key + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(filename, 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "    \n",
    "# save descriptions\n",
    "save_doc(descriptions, 'D:/Program/dataset/Flickr8K/descriptions.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the Photos\n",
    "\n",
    "We will use a pre-trained model to interpret the content of the photos.\n",
    "\n",
    "There are many models to choose from. In this case, we will use the Oxford Visual Geometry Group or VGG model that won the ImageNet competition in 2014.Keras provides this pre-trained model directly.\n",
    "\n",
    "We could use this model as part of a broader image caption model. The problem is, it is a large model and running each photo through the network every time we want to test a new language model configuration (downstream) is redundant.\n",
    "\n",
    "Instead, we can pre-compute the “photo features” using the pre-trained model and save them to file. We can then load these features later and feed them into our model as the interpretation of a given photo in the dataset. It is no different to running the photo through the full VGG model, it is just that we will have done it once in advance.\n",
    "\n",
    "This is an optimization that will make training our models faster and consume less memory.\n",
    "\n",
    "We can load the VGG model in Keras using the VGG class. We will load the model without the top; this means without the layers at the end of the network that are used to interpret the features extracted from the input and turn them into a class prediction. We are not interested in the image net classification of the photos and we will train our own interpretation of the image features.\n",
    "\n",
    "Below is a function named extract_features() that given a directory name will load each photo, prepare it for VGG and collect the predicted features from the VGG model. The image features are a 3-dimensional array with the shape (7, 7, 512).\n",
    "\n",
    "The function returns a dictionary of image identifier to image features.\n",
    "We can call this function to prepare the photo data for testing our models, then save the resulting dictionary to a file named *features.pkl*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "=================================================================\n",
      "Total params: 14,714,688\n",
      "Trainable params: 14,714,688\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      ">1107471216_4336c9b328.jpg\n",
      ">124195430_d14028660f.jpg\n",
      ">1332208215_fa824f6659.jpg\n",
      ">1403414927_5f80281505.jpg\n",
      ">1463732807_0cdf4f22c7.jpg\n",
      ">1562478713_505ab6d924.jpg\n",
      ">1773928579_5664a810dc.jpg\n",
      ">1947351225_288d788983.jpg\n",
      ">2065875490_a46b58c12b.jpg\n",
      ">2098418613_85a0c9afea.jpg\n",
      ">2151056407_c9c09b0a02.jpg\n",
      ">2198511848_311d8a8c2f.jpg\n",
      ">2244171992_a4beb04d8e.jpg\n",
      ">2273591668_069dcb4641.jpg\n",
      ">2308108566_2cba6bca53.jpg\n",
      ">2350400382_ced2b6c91e.jpg\n",
      ">2392625002_83a5a0978f.jpg\n",
      ">241347441_d3dd9b129f.jpg\n",
      ">244368383_e90b6b2f20.jpg\n",
      ">2470519275_65725fd38d.jpg\n",
      ">2503629305_055e9ec4b1.jpg\n",
      ">253762507_9c3356c2f6.jpg\n",
      ">2567035103_3511020c8f.jpg\n",
      ">260231029_966e2f1727.jpg\n",
      ">2635908229_b9fc90d3fb.jpg\n",
      ">267015208_d80b3eb94d.jpg\n",
      ">2701895972_8605c4e038.jpg\n",
      ">2735979477_eef7c680f9.jpg\n",
      ">2769731772_18c44c18e2.jpg\n",
      ">2814028429_561a215259.jpg\n",
      ">2846843520_b0e6211478.jpg\n",
      ">2872806249_00bea3c4e7.jpg\n",
      ">2894850774_2d530040a1.jpg\n",
      ">2924483864_cfdb900a13.jpg\n",
      ">2950393735_9969c4ec59.jpg\n",
      ">2987195421_e830c59fb6.jpg\n",
      ">3016178284_ec50a09e8c.jpg\n",
      ">3042380610_c5ea61eef8.jpg\n",
      ">3070011270_390e597783.jpg\n",
      ">3094278545_febac56382.jpg\n",
      ">3117336911_a729f42869.jpg\n",
      ">3143155555_32b6d24f34.jpg\n",
      ">316577571_27a0e0253e.jpg\n",
      ">3185695861_86152b2755.jpg\n",
      ">3208999896_dab42dc40b.jpg\n",
      ">3225310099_d8e419ba56.jpg\n",
      ">3248220732_0f173fc197.jpg\n",
      ">3264350290_f50494e835.jpg\n",
      ">3286045254_696c6b15bd.jpg\n",
      ">3309578722_1765d7d1af.jpg\n",
      ">3331190056_09f4ca9fd2.jpg\n",
      ">3349308309_92cff519f3.jpg\n",
      ">3366571152_20afb88ac1.jpg\n",
      ">3393035454_2d2370ffd4.jpg\n",
      ">3414734842_beb543f400.jpg\n",
      ">3429641260_2f035c1813.jpg\n",
      ">3449114979_6cdc3e8da8.jpg\n",
      ">3468346269_9d162aacfe.jpg\n",
      ">3487131146_9d3aca387a.jpg\n",
      ">3508882611_3947c0dbf5.jpg\n",
      ">3528902357_be2357a906.jpg\n",
      ">354642192_3b7666a2dd.jpg\n",
      ">3564436847_57825db87d.jpg\n",
      ">358607894_5abb1250d3.jpg\n",
      ">3607405494_0df89110a6.jpg\n",
      ">3628698119_5566769777.jpg\n",
      ">3647826834_dc63e21bd0.jpg\n",
      ">3671935691_57bdd0e778.jpg\n",
      ">3692836015_d11180727b.jpg\n",
      ">3717809376_f97611ab84.jpg\n",
      ">381052465_722e00807b.jpg\n",
      ">412101267_7257e6d8c0.jpg\n",
      ">441398149_297146e38d.jpg\n",
      ">470887781_faae5dae83.jpg\n",
      ">494792770_2c5f767ac0.jpg\n",
      ">517094985_4b9e926936.jpg\n",
      ">539751252_2bd88c456b.jpg\n",
      ">619169586_0a13ee7c21.jpg\n",
      ">758921886_55a351dd67.jpg\n",
      ">887108308_2da97f15ef.jpg\n",
      "Extracted Features: 8091\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "from os import listdir\n",
    "from pickle import dump\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input\n",
    "\n",
    "# extract features from each photo in the directory\n",
    "def extract_features(directory):\n",
    "    # load the model\n",
    "    in_layer = Input(shape=(224, 224, 3))\n",
    "    model = VGG16(include_top=False, input_tensor=in_layer)\n",
    "    print(model.summary())\n",
    "    # extract features from each photo\n",
    "    features = dict()\n",
    "    for name in listdir(directory):\n",
    "        # load an image from file\n",
    "        filename = directory + '/' + name\n",
    "        image = load_img(filename, target_size=(224, 224))\n",
    "        # convert the image pixels to a numpy array\n",
    "        image = img_to_array(image)\n",
    "        # reshape data for the model\n",
    "        image = image.reshape((1, image.shape[0], image.shape[1], image.shape[2]))\n",
    "        # prepare the image for the VGG model\n",
    "        image = preprocess_input(image)\n",
    "        # get features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        image_id = name.split('.')[0]\n",
    "        # store feature\n",
    "        features[image_id] = feature\n",
    "        \n",
    "        if len(features)%100==0:\n",
    "            print('>{}'.format(name))\n",
    "    return features\n",
    "\n",
    "# extract features from all images\n",
    "directory = 'D:/Program/dataset/Flickr8K/Flicker8k_Dataset'\n",
    "features = extract_features(directory)\n",
    "print('Extracted Features: {}'.format(len(features)))\n",
    "# save to file\n",
    "dump(features, open('D:/Program/dataset/Flickr8K/features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Running this data preparation step may take a while depending on your hardware, perhaps one hour on the CPU with a modern workstation.\n",
    "\n",
    "At the end of the run, you will have the extracted features stored in *features.pkl* for later use.\n",
    "\n",
    "# Baseline Caption Generation Model\n",
    "\n",
    "In this section, we will define a baseline model for generating captions for photos and how to evaluate it so that it can be compared to variations on this baseline.\n",
    "\n",
    "This section is divided into 6 parts:\n",
    "\n",
    "1. Load Data.\n",
    "2. Fit Model.\n",
    "3. Evaluate Model.\n",
    "4. Complete Example\n",
    "5. “A” versus “A” Test\n",
    "6. Generate Photo Captions\n",
    "\n",
    "## Load Data\n",
    "\n",
    "We are not going to fit the model on all of the caption data, or even on a large sample of the data.\n",
    "\n",
    "In this tutorial, we are interested in quickly testing a suite of different configurations of a caption model to see what works on this data. That means we need the evaluation of one model configuration to happen quickly. Toward this end, we will train the models on 100 photographs and captions, then evaluate them on both the training dataset and on a new test set of 100 photographs and captions.\n",
    "\n",
    "First, we need to load a pre-defined subset of photographs. The provided dataset has separate sets for train, test, and development, which are really just different groups of photo identifiers. We will load the development set and use the first 100 identifiers for train and the second 100 (e.g. from 100 to 200) as the test set.\n",
    "\n",
    "The function `load_set()` below will load a pre-defined set of identifiers, and we will call it with the *Flickr_8k.devImages.txt* filename as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a pre-defined list of photo identifiers\n",
    "def load_set(filename):\n",
    "    doc = load_doc(filename)\n",
    "    dataset = list()\n",
    "    # process line by line\n",
    "    for line in doc.split('\\n'):\n",
    "        # skip empty lines\n",
    "        if len(line) < 1:\n",
    "            continue\n",
    "        # get the image identifier\n",
    "        identifier = line.split('.')[0]\n",
    "        dataset.append(identifier)\n",
    "    return set(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we need to split the set into train and test sets.\n",
    "\n",
    "We will start by ordering the identifiers by sorting them to ensure we always split them consistently across machines and runs, then take the first 100 for train and the next 100 for test.\n",
    "\n",
    "The `train_test_split()` function below will create this split given the loaded set of identifiers as input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split a dataset into train/test elements\n",
    "def train_test_split(dataset):\n",
    "    # order keys so the split is consistent\n",
    "    ordered = sorted(dataset)\n",
    "    # return split dataset as two new sets\n",
    "    return set(ordered[:100]), set(ordered[100:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now, we can load the photo descriptions using the pre-defined set of train or test identifiers.\n",
    "\n",
    "Below is the function `load_clean_descriptions()` that loads the cleaned text descriptions from *descriptions.txt* for a given set of identifiers and returns a dictionary of identifier to text.\n",
    "\n",
    "The model we will develop will generate a caption given a photo, and the caption will be generated one word at a time. The sequence of previously generated words will be provided as input. Therefore, we will need a ***first word*** to kick-off the generation process and a ***last word*** to signal the end of the caption. We will use the strings ***startseq*** and ***endseq*** for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load clean descriptions into memory\n",
    "def load_clean_descriptions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    descriptions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # split id from description\n",
    "        image_id, image_desc = tokens[0], tokens[1:]\n",
    "        # skip images not in the set\n",
    "        if image_id in dataset:\n",
    "            # store\n",
    "            descriptions[image_id] = 'startseq ' + ' '.join(image_desc) + ' endseq'\n",
    "    return descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Next, we can load the photo features for a given dataset.\n",
    "\n",
    "Below defines a function named `load_photo_features()` that loads the entire set of photo descriptions, then returns the subset of interest for a given set of photo identifiers. This is not very efficient as the loaded dictionary of all photo features is about 700 Megabytes. Nevertheless, this will get us up and running quickly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pickle import load\n",
    "\n",
    "# load photo features\n",
    "def load_photo_features(filename, dataset):\n",
    "    # load all features\n",
    "    all_features = load(open(filename, 'rb'))\n",
    "    # filter features\n",
    "    features = {k: all_features[k] for k in dataset}\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset: 1000\n",
      "Train=100, Test=100\n",
      "Descriptions: train=100, test=100\n",
      "Photos: train=100, test=100\n"
     ]
    }
   ],
   "source": [
    "# load dev set\n",
    "filename = 'D:/Program/dataset/Flickr8K/Flickr8k_text/Flickr_8k.devImages.txt'\n",
    "dataset = load_set(filename)\n",
    "print('Dataset: {}'.format(len(dataset)))\n",
    "# train-test split\n",
    "train, test = train_test_split(dataset)\n",
    "print('Train={}, Test={}'.format(len(train), len(test)))\n",
    "# descriptions\n",
    "train_descriptions = load_clean_descriptions('D:/Program/dataset/Flickr8K/descriptions.txt', train)\n",
    "test_descriptions = load_clean_descriptions('D:/Program/dataset/Flickr8K/descriptions.txt', test)\n",
    "print('Descriptions: train={}, test={}'.format(len(train_descriptions), len(test_descriptions)))\n",
    "# photo features\n",
    "train_features = load_photo_features('D:/Program/dataset/Flickr8K/features.pkl', train)\n",
    "test_features = load_photo_features('D:/Program/dataset/Flickr8K/features.pkl', test)\n",
    "print('Photos: train={}, test={}'.format(len(train_features), len(test_features)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset copyright\n",
    "\n",
    "Please cite M. Hodosh, P. Young and J. Hockenmaier (2013) \"Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics\", Journal of Artificial Intelligence Research, Volume 47, pages 853-899 http://www.jair.org/papers/paper3994.html when discussing our results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
