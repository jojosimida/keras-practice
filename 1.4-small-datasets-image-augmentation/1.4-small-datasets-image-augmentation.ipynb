{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to train a small data set\n",
    "\n",
    "Training a model of image classification with very little data is a common practice.\n",
    "\n",
    "### Strategy:\n",
    "\n",
    "* Train a small model from scratch\n",
    "* Overfitting -> regularization, Dropiut\n",
    "* Overfitting -> Data Augmentation\n",
    "* Pre-trained models were used for feature extraction\n",
    "* Fine-tune the pre-trained model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform: Windows-10-10.0.16299-SP0\n",
      "Tensorflow version: 1.4.0\n",
      "Keras version: 2.1.2\n"
     ]
    }
   ],
   "source": [
    "import platform\n",
    "import tensorflow\n",
    "import keras\n",
    "print(\"Platform: {}\".format(platform.platform()))\n",
    "print(\"Tensorflow version: {}\".format(tensorflow.__version__))\n",
    "print(\"Keras version: {}\".format(keras.__version__))\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import numpy as np\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data set description\n",
    "\n",
    "Kaggle\n",
    "\n",
    "data set: Dogs vs. Cats [download](https://www.kaggle.com/c/dogs-vs-cats/data)\n",
    "\n",
    "![dataset](https://imgur.com/0Il4OZF.png)\n",
    "\n",
    "The original dataset contains images of 25,000 dogs and cats (12,500 per category) and a size of 543MB (compressed). \n",
    "\n",
    "After downloading and extracting, we will create a new data set containing three subsets: a set of **Training sets** containing **1000** samples per class, a **Validation set** of **500** samples per set, and the last one containing **500** for each class **Test set** of a sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT_DIR = os.getcwd()\n",
    "original_dataset_dir = \"D:/Program/dataset/Dogs_vs_Cats\"\n",
    "ori_train = os.path.join(original_dataset_dir,\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def create_directory(datapath, subname):\n",
    "    child_datapath = os.path.join(datapath, subname)\n",
    "    if not os.path.exists(child_datapath): \n",
    "        os.mkdir(child_datapath)\n",
    "        \n",
    "    return child_datapath\n",
    "\n",
    "\n",
    "# Create a directory to store the data set\n",
    "base_dir = create_directory(original_dataset_dir, \"cats_and_dogs_small\")\n",
    "\n",
    "# Training materials directory\n",
    "train_dir = create_directory(base_dir, 'train')\n",
    "\n",
    "# Validation data directory\n",
    "validation_dir = create_directory(base_dir, 'validation')\n",
    "\n",
    "# Test data directory\n",
    "test_dir = create_directory(base_dir, 'test')\n",
    "\n",
    "\n",
    "# Cat's picture of the training information directory\n",
    "train_cats_dir = create_directory(train_dir, 'cats')\n",
    "\n",
    "# Dog pictures of training materials directory\n",
    "train_dogs_dir = create_directory(train_dir, 'dogs')\n",
    "\n",
    "# Cat's picture of the authentication data directory\n",
    "validation_cats_dir = create_directory(validation_dir, 'cats')\n",
    "\n",
    "# Dog's picture verification information directory\n",
    "validation_dogs_dir = create_directory(validation_dir, 'dogs')\n",
    "\n",
    "# Cat's picture of the test data directory\n",
    "test_cats_dir = create_directory(test_dir, 'cats')\n",
    "\n",
    "# Dog's picture of the test data directory\n",
    "test_dogs_dir = create_directory(test_dir, 'dogs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copy first 1000 cat images to train_cats_dir complete!\n",
      "Copy next 500 cat images to validation_cats_dir complete!\n",
      "Copy next 500 cat images to test_cats_dir complete!\n",
      "Copy first 1000 cat images to train_dogs_dir complete!\n",
      "Copy next 500 cat images to validation_dogs_dir complete!\n",
      "Copy next 500 cat images to test_dogs_dir complete!\n"
     ]
    }
   ],
   "source": [
    "def copy_ori_pic_to_small_file(fnames, dst_path, ori_train=ori_train):\n",
    "    for fname in fnames:\n",
    "        src = os.path.join(ori_train, fname)\n",
    "        dst = os.path.join(dst_path, fname)\n",
    "        if not os.path.exists(dst):\n",
    "            shutil.copyfile(src, dst)\n",
    "            \n",
    "# Copy the first 1000 pictures of cats to train_cats_dir    \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000)]\n",
    "copy_ori_pic_to_small_file(fnames, train_cats_dir)\n",
    "print('Copy first 1000 cat images to train_cats_dir complete!')\n",
    "\n",
    "# Copy the next 500 pictures of cats to validation_cats_dir \n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "copy_ori_pic_to_small_file(fnames, validation_cats_dir)\n",
    "print('Copy next 500 cat images to validation_cats_dir complete!')\n",
    "\n",
    "# Copy the next 500 pictures of cats to test_cats_dir\n",
    "fnames = ['cat.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "copy_ori_pic_to_small_file(fnames, test_cats_dir)\n",
    "print('Copy next 500 cat images to test_cats_dir complete!')\n",
    "\n",
    "\n",
    "# Copy the first 1000 pictures of dogs to train_dogs_dir    \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000)]\n",
    "copy_ori_pic_to_small_file(fnames, train_dogs_dir)\n",
    "print('Copy first 1000 cat images to train_dogs_dir complete!')\n",
    "\n",
    "# Copy the next 500 pictures of dogs to validation_dogs_dir \n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1000, 1500)]\n",
    "copy_ori_pic_to_small_file(fnames, validation_dogs_dir)\n",
    "print('Copy next 500 cat images to validation_dogs_dir complete!')\n",
    "\n",
    "# Copy the next 500 pictures of dogs to test_dogs_dir\n",
    "fnames = ['dog.{}.jpg'.format(i) for i in range(1500, 2000)]\n",
    "copy_ori_pic_to_small_file(fnames, test_dogs_dir)\n",
    "print('Copy next 500 cat images to test_dogs_dir complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total training cat images: 1000\n",
      "total training dog images: 1000\n",
      "total validation cat images: 500\n",
      "total validation dog images: 500\n",
      "total test cat images: 500\n",
      "total test dog images: 500\n"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_cats_dir)))\n",
    "print('total training dog images:', len(os.listdir(train_dogs_dir)))\n",
    "print('total validation cat images:', len(os.listdir(validation_cats_dir)))\n",
    "print('total validation dog images:', len(os.listdir(validation_dogs_dir)))\n",
    "print('total test cat images:', len(os.listdir(test_cats_dir)))\n",
    "print('total test dog images:', len(os.listdir(test_dogs_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "### Strategy:\n",
    "\n",
    "* Read into the image file.\n",
    "* Decodes the JPEG content into pixels for the RGB grid.\n",
    "* Convert it to a floating-point tensor.\n",
    "* Rescale pixel values (between 0 and 255) to [0,1] intervals (as you know, neural networks prefer to handle small input values).\n",
    "\n",
    "reference: \n",
    "[ImageDataGenerator](https://keras-cn.readthedocs.io/en/latest/preprocessing/image/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2000 images belonging to 2 classes.\n",
      "Found 1000 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# All images will be normalized : Rescaled by 1./255\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# Directly from the file directory to read the image file information\n",
    "train_generator = train_datagen.flow_from_directory( \n",
    "        # This is the directory of image data\n",
    "        train_dir,\n",
    "        # All image sizes will be resized to 150x150\n",
    "        target_size=(150, 150),\n",
    "        # Each time a batch of 20 images is generated\n",
    "        batch_size=20,\n",
    "        # Since this is a binary classification problem, the lable value of y is also converted to a binary label\n",
    "        class_mode='binary')\n",
    "\n",
    "\n",
    "validation_generator = test_datagen.flow_from_directory(\n",
    "        validation_dir,\n",
    "        target_size=(150, 150),\n",
    "        batch_size=20,\n",
    "        class_mode='binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br><br>\n",
    "It produces batch tensions of 150 × 150 RGB images (shape \"(20,150,150,3)\") and binary labels (shape \"(20,)\"). \n",
    "\n",
    "20 is the number of samples in each batch (batch size). \n",
    "\n",
    "Note that the generator can generate these lots without any restrictions: because it simply iterates over the images that exist in the destination folder. Therefore, we need to \"break\" the iteration loop at some point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (20, 150, 150, 3)\n",
      "labels batch shape: (20,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model\n",
    "\n",
    "* Input:150x150\n",
    "* Conv2D:reLU\n",
    "* MaxPooling2D\n",
    "* 7x7 before the Flatten layer \n",
    "* flatten\n",
    "* 512,relu\n",
    "* 1,sigmoid\n",
    "<br><br>\n",
    "\n",
    "The depth of the feature map gradually increases over the network (from 32 to 128), while the size of the feature map is decreasing (from 148x148 to 7x7). \n",
    "\n",
    "This is a pattern you will see in almost all convnets constructs.\n",
    "<br><br>\n",
    "Since we are dealing with binary classification problems, we ended the network with a neuron (a Dense of size 1) and a sigmoid activation function. \n",
    "\n",
    "This neuron will be used to see the probability that the image belongs to that class or another class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_9 (Conv2D)            (None, 150, 150, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 75, 75, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 75, 75, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 37, 37, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 37, 37, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_11 (MaxPooling (None, 18, 18, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_12 (Conv2D)           (None, 18, 18, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_12 (MaxPooling (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_3 (Flatten)          (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 512)               5308928   \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 5,550,273\n",
      "Trainable params: 5,550,273\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.utils import plot_model\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu',input_shape=(150, 150, 3),padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu',padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(128, (3, 3), activation='relu',padding='same'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "# model.compile(loss='binary_crossentropy',\n",
    "#               optimizer=optimizers.RMSprop(lr=1e-4),\n",
    "#               metrics=['acc'])\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n",
    "##  About fit_generator in Chinese\n",
    "\n",
    "讓我們將模型與使用圖像張量產生器的數據進行訓練。我們使用fit_generator方法。\n",
    "\n",
    "因為數據是可以無休止地持續生成，所以圖像張量產生器需要知道在一個訓練循環(epoch)要從圖像張量產生器中抽取多少個資料。這是steps_per_epoch參數的作用：在從生成器中跑過steps_per_epoch批次之後，即在運行steps_per_epoch梯度下降步驟之後，訓練過程將轉到下一個循環(epoch)。在我們的情況下，批次是20個樣本，所以它需要100次，直到我們的模型讀進了2000個目標樣本。\n",
    "\n",
    "當使用fit_generator時，可以傳遞一個validation_data參數，就像fit方法一樣。重要的是，這個參數被允許作為數據生成器本身，但它也可以是一個Numpy數組的元組。如果您將生成器傳遞為validation_data，那麼這個生成器有望不斷生成一批驗證數據，因此您還應該指定validation_steps參數，該參數告訴進程從驗證生成器中抽取多少批次以進行評估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "100/100 [==============================] - 15s 151ms/step - loss: 0.6983 - acc: 0.5160 - val_loss: 0.6882 - val_acc: 0.5860\n",
      "Epoch 2/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.6893 - acc: 0.5260 - val_loss: 0.6921 - val_acc: 0.5450\n",
      "Epoch 3/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.6863 - acc: 0.5585 - val_loss: 0.6717 - val_acc: 0.5830\n",
      "Epoch 4/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.6700 - acc: 0.5890 - val_loss: 0.6520 - val_acc: 0.6010\n",
      "Epoch 5/30\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.6403 - acc: 0.6340 - val_loss: 0.6734 - val_acc: 0.5700\n",
      "Epoch 6/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.6087 - acc: 0.6710 - val_loss: 0.6196 - val_acc: 0.6600\n",
      "Epoch 7/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.5714 - acc: 0.6980 - val_loss: 0.6604 - val_acc: 0.6400\n",
      "Epoch 8/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.5180 - acc: 0.7405 - val_loss: 0.5970 - val_acc: 0.6910\n",
      "Epoch 9/30\n",
      "100/100 [==============================] - 10s 99ms/step - loss: 0.4597 - acc: 0.7805 - val_loss: 0.6387 - val_acc: 0.6850\n",
      "Epoch 10/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.3851 - acc: 0.8230 - val_loss: 0.6154 - val_acc: 0.7000\n",
      "Epoch 11/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.3003 - acc: 0.8700 - val_loss: 0.8606 - val_acc: 0.6770\n",
      "Epoch 12/30\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.2422 - acc: 0.8995 - val_loss: 0.8868 - val_acc: 0.7100\n",
      "Epoch 13/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.1478 - acc: 0.9430 - val_loss: 1.1354 - val_acc: 0.6940\n",
      "Epoch 14/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.0873 - acc: 0.9685 - val_loss: 1.3907 - val_acc: 0.6900\n",
      "Epoch 15/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.0716 - acc: 0.9740 - val_loss: 1.3047 - val_acc: 0.7100\n",
      "Epoch 16/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.0283 - acc: 0.9915 - val_loss: 1.7551 - val_acc: 0.6840\n",
      "Epoch 17/30\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 0.0704 - acc: 0.9740 - val_loss: 1.6275 - val_acc: 0.6840\n",
      "Epoch 18/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 0.0388 - acc: 0.9855 - val_loss: 1.6176 - val_acc: 0.7010\n",
      "Epoch 19/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 0.0496 - acc: 0.9850 - val_loss: 1.6381 - val_acc: 0.6910\n",
      "Epoch 20/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 0.0060 - acc: 0.9990 - val_loss: 1.9812 - val_acc: 0.6880\n",
      "Epoch 21/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 9.3368e-04 - acc: 1.0000 - val_loss: 2.0350 - val_acc: 0.6980\n",
      "Epoch 22/30\n",
      "100/100 [==============================] - 10s 98ms/step - loss: 3.4420e-04 - acc: 1.0000 - val_loss: 2.1364 - val_acc: 0.7010\n",
      "Epoch 23/30\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 1.8803e-04 - acc: 1.0000 - val_loss: 2.2331 - val_acc: 0.7010\n",
      "Epoch 24/30\n",
      "100/100 [==============================] - 9s 95ms/step - loss: 1.2468e-04 - acc: 1.0000 - val_loss: 2.2747 - val_acc: 0.6940\n",
      "Epoch 25/30\n",
      "100/100 [==============================] - 10s 97ms/step - loss: 8.4018e-05 - acc: 1.0000 - val_loss: 2.3507 - val_acc: 0.7000\n",
      "Epoch 26/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 6.0296e-05 - acc: 1.0000 - val_loss: 2.3422 - val_acc: 0.7030\n",
      "Epoch 27/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 4.4884e-05 - acc: 1.0000 - val_loss: 2.4018 - val_acc: 0.7040\n",
      "Epoch 28/30\n",
      "100/100 [==============================] - 10s 96ms/step - loss: 3.6035e-05 - acc: 1.0000 - val_loss: 2.4229 - val_acc: 0.7030\n",
      "Epoch 29/30\n",
      "100/100 [==============================] - 10s 95ms/step - loss: 2.6770e-05 - acc: 1.0000 - val_loss: 2.4466 - val_acc: 0.7030\n",
      "Epoch 30/30\n",
      "100/100 [==============================] - 9s 94ms/step - loss: 2.0511e-05 - acc: 1.0000 - val_loss: 2.4889 - val_acc: 0.7040\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(\n",
    "          train_generator,\n",
    "          steps_per_epoch=100,\n",
    "          epochs=30,\n",
    "          validation_data=validation_generator,\n",
    "          validation_steps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save('D:/Program/dataset/Dogs_vs_Cats/model/cats_and_dogs_small_1.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
