{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import jieba\n",
    "import string\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.layers import Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Param():\n",
    "    batch_size = 32\n",
    "    n_epoch = 100\n",
    "    learning_rate = 0.01\n",
    "    decay_steps = 1000\n",
    "    decay_rate = 0.9\n",
    "    grad_clip = 5\n",
    "    state_size = 100\n",
    "    num_layers = 3\n",
    "    seq_length = 20\n",
    "    log_dir = './logs'\n",
    "    metadata = 'metadata.tsv'\n",
    "    gen_num = 500 # how many chars to generate\n",
    "    \n",
    "    remove_word = ['!', '(', ')', '*', '+', ',', '-', '.',\n",
    "                   '...', '......', '............', '/','<',\n",
    "                   '>', '?','[', '\\\\', ']', '`','~', '·',\n",
    "                   '…', '☆', '\\u3000', '。', '〇', '《', '》',\n",
    "                   '〖', '〗', 'ー', 'ㄇ', 'ㄈ', 'ㄌ', 'ㄒ', 'ㄙ','！',\n",
    "                   'ㄚ', 'ㄟ', 'ㄡ','（','）','，','＜','＞','？','～']   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, datafiles, args):\n",
    "        self.seq_length = args.seq_length\n",
    "        self.batch_size = args.batch_size\n",
    "        self.remove_word = args.remove_word\n",
    "        self.remove_word = ''.join(self.remove_word)\n",
    "        \n",
    "        with open(datafiles, encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        \n",
    "        self.seg_list = list(jieba.cut(self.data, cut_all=False))\n",
    "        \n",
    "        table = str.maketrans('','',self.remove_word)\n",
    "        self.seg_list = [w.translate(table) for w in self.seg_list]\n",
    "                \n",
    "        # total data length\n",
    "        self.total_len = len(self.seg_list)  \n",
    "        self.words = list(set(self.seg_list))\n",
    "        self.words.sort()\n",
    "        print('Total length: {}'.format(self.total_len))\n",
    "        \n",
    "        # vocabulary\n",
    "        self.vocab_size = len(self.words)  # vocabulary size\n",
    "        print('Vocabulary Size:', self.vocab_size)\n",
    "        \n",
    "        # dictionary\n",
    "        self.char2id_dict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.id2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "        \n",
    "        # pointer position to generate current batch\n",
    "        self._pointer = 0\n",
    "        # save metadata file\n",
    "        self.save_metadata(args.metadata)\n",
    "        \n",
    "    def char2id(self, c):\n",
    "        return self.char2id_dict[c]\n",
    "    \n",
    "    def id2char(self, id):\n",
    "        return self.id2char_dict[id]\n",
    "    \n",
    "    def save_metadata(self, file):\n",
    "        with open(file, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write('id\\tchar\\n')\n",
    "            for i in range(self.vocab_size):\n",
    "                c = self.id2char(i)\n",
    "                f.write('{}\\t{}\\n'.format(i, c))\n",
    "                \n",
    "#     def create_tokenizer(self):\n",
    "#         tokenizer = Tokenizer()\n",
    "#         tokenizer.fit_on_texts(self.seg_list)\n",
    "#         return tokenizer\n",
    "    \n",
    "    def next_batch(self):\n",
    "        x_batches = []\n",
    "        y_batches = []\n",
    "        for i in range(self.batch_size):\n",
    "            if self._pointer + self.seq_length + 1 >= self.total_len:\n",
    "                self._pointer = 0\n",
    "\n",
    "            bx = self.seg_list[self._pointer: self._pointer + self.seq_length]\n",
    "            by = self.seg_list[self._pointer +\n",
    "                           1: self._pointer + self.seq_length + 1]\n",
    "\n",
    "            # update pointer position\n",
    "            self._pointer += self.seq_length  \n",
    "\n",
    "            # convert to ids\n",
    "            bx = [self.char2id(c) for c in bx]\n",
    "            by = [self.char2id(c) for c in by]\n",
    "\n",
    "#             by = to_categorical(by, num_classes=self.vocab_size)[0]\n",
    "\n",
    "            x_batches.append(bx)\n",
    "            y_batches.append(by)\n",
    "        \n",
    "        return x_batches, y_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def define_model(args, data):\n",
    "\n",
    "    # embedding\n",
    "    inputs = Input(shape=(args.seq_length,))\n",
    "    emb2 = Embedding(data.vocab_size, 50, mask_zero=True)(inputs)\n",
    "    emb3 = LSTM(256)(emb2)\n",
    "\n",
    "    # language model (decoder)\n",
    "#     lm2 = LSTM(256)(emb3)\n",
    "    lm3 = Dense(500, activation='relu')(emb3)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lm3)\n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    print(model.summary())\n",
    "    plot_model(model, show_shapes=True, to_file='plot.png')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from D:\\Program\\model\\jieba\\dict.txt.big.txt ...\n",
      "Loading model from cache C:\\Users\\user\\AppData\\Local\\Temp\\jieba.u5f50f2b7ab381204f5f6032f217b280e.cache\n",
      "Loading model cost 1.322 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 43933\n",
      "Vocabulary Size: 5801\n"
     ]
    }
   ],
   "source": [
    "lyrics = \"jaychou_lyrics_traditional.txt\"\n",
    "JIEBA_DICTFILE_PATH = \"D:/Program/model/jieba/dict.txt.big.txt\"\n",
    "jieba.set_dictionary(JIEBA_DICTFILE_PATH) \n",
    "\n",
    "args = Param()\n",
    "data = DataGenerator(lyrics, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 5801)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([[2192,\n",
       "   2889,\n",
       "   3806,\n",
       "   1,\n",
       "   2192,\n",
       "   1182,\n",
       "   535,\n",
       "   5633,\n",
       "   1686,\n",
       "   1006,\n",
       "   1,\n",
       "   2192,\n",
       "   1182,\n",
       "   535,\n",
       "   4566,\n",
       "   1365,\n",
       "   170,\n",
       "   1,\n",
       "   4566,\n",
       "   1365]],\n",
       " [array([ 0.,  0.,  0., ...,  0.,  0.,  0.])])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.next_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = define_model(args, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
