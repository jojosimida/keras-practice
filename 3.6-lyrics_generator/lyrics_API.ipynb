{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import string\n",
    "from hanziconv import HanziConv\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import plot_model, to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense, LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.models import load_model\n",
    "\n",
    "from IPython.display import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Param():\n",
    "    batch_size = 32\n",
    "    n_epoch = 50\n",
    "    seq_length = 20\n",
    "    metadata = 'metadata.tsv'\n",
    "    n_step = 3\n",
    "    n_batches_per_epoch = 500\n",
    "    lyrics_long = 360\n",
    "\n",
    "    \n",
    "    remove_word = ['!', '(', ')', '*', '+', ',', '-', '.',\n",
    "                   '...', '......', '............', '/','<',\n",
    "                   '>', '?','[', '\\\\', ']', '`','~', '·',\n",
    "                   '…', '☆', '\\u3000', '。', '〇', '《', '》',\n",
    "                   '〖', '〗', 'ー', 'ㄇ', 'ㄈ', 'ㄌ', 'ㄒ', 'ㄙ','！',\n",
    "                   'ㄚ', 'ㄟ', 'ㄡ','（','）','，','＜','＞','？','～']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class DataGenerator():\n",
    "    def __init__(self, datafiles, args):\n",
    "        self.seq_length = args.seq_length\n",
    "        self.batch_size = args.batch_size\n",
    "        self.n_step = args.n_step\n",
    "        \n",
    "        self.remove_word = args.remove_word\n",
    "        self.remove_word = ''.join(self.remove_word)        \n",
    "        \n",
    "        with open(datafiles, encoding='utf-8') as f:\n",
    "            self.data = f.read()\n",
    "        \n",
    "        table = str.maketrans('','',self.remove_word)\n",
    "        self.data = [w.translate(table) for w in self.data]\n",
    "                \n",
    "        # total data length\n",
    "        self.total_len = len(self.data)  \n",
    "        self.words = list(set(self.data))\n",
    "        self.words.sort()\n",
    "        print('Total length: {}'.format(self.total_len))\n",
    "        \n",
    "        # vocabulary\n",
    "        self.vocab_size = len(self.words)  # vocabulary size\n",
    "        print('Vocabulary Size:', self.vocab_size)\n",
    "        \n",
    "        # dictionary\n",
    "        self.char2id_dict = {w: i for i, w in enumerate(self.words)}\n",
    "        self.id2char_dict = {i: w for i, w in enumerate(self.words)}\n",
    "        \n",
    "        # pointer position to generate current batch\n",
    "        self._pointer = 0\n",
    "#         # save metadata file\n",
    "#         self.save_metadata(args.metadata)\n",
    "        \n",
    "        self.max_iter = args.n_epoch * \\\n",
    "            (self.total_len // args.seq_length) // args.batch_size\n",
    "        \n",
    "    def char2id(self, c):\n",
    "        return self.char2id_dict[c]\n",
    "    \n",
    "    def id2char(self, id):\n",
    "        return self.id2char_dict[id]\n",
    "    \n",
    "    def save_metadata(self, file):\n",
    "        with open(file, 'w', encoding=\"utf-8\") as f:\n",
    "            f.write('id\\tchar\\n')\n",
    "            for i in range(self.vocab_size):\n",
    "                c = self.id2char(i)\n",
    "                f.write('{}\\t{}\\n'.format(i, c))\n",
    "                \n",
    "#     def create_tokenizer(self):\n",
    "#         tokenizer = Tokenizer()\n",
    "#         tokenizer.fit_on_texts(self.seg_list)\n",
    "#         return tokenizer\n",
    "    \n",
    "    def next_batch(self):\n",
    "        x_batches = []\n",
    "        y_batches = []\n",
    "        for i in range(self.batch_size):\n",
    "            if self._pointer + self.seq_length + 1 >= self.total_len:\n",
    "                self._pointer = 0\n",
    "\n",
    "            bx = self.data[self._pointer: self._pointer + self.seq_length]\n",
    "#             by = self.seg_list[self._pointer +\n",
    "#                            1: self._pointer + self.seq_length + 1]\n",
    "            by = self.data[self._pointer + self.seq_length]\n",
    "\n",
    "            # update pointer position\n",
    "            self._pointer += 1\n",
    "\n",
    "            # convert to ids\n",
    "            bx = [self.char2id(c) for c in bx]\n",
    "            by = [self.char2id(by)]\n",
    "\n",
    "            by = to_categorical(by, num_classes=self.vocab_size)[0]\n",
    "\n",
    "            x_batches.append(bx)\n",
    "            y_batches.append(by)\n",
    "        \n",
    "        return x_batches, y_batches\n",
    "    \n",
    "    \n",
    "    def data_generator(self):\n",
    "        while 1:\n",
    "        # loop over photo identifiers in the dataset\n",
    "\n",
    "            for i in range(0, self.max_iter, self.n_step):\n",
    "                XSeq, y = list(), list()\n",
    "                for j in range(i, min(self.max_iter, i+self.n_step)):\n",
    "\n",
    "                    # generate input-output pairs\n",
    "                    in_seq, out_word = self.next_batch()\n",
    "\n",
    "                    for k in range(len(in_seq)):\n",
    "                        XSeq.append(in_seq[k])\n",
    "                        y.append(out_word[k])\n",
    "                # yield this batch of samples to the model\n",
    "                yield [np.array(XSeq), np.array(y)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 65697\n",
      "Vocabulary Size: 2445\n"
     ]
    }
   ],
   "source": [
    "PATH = \"D:/Program/dataset/lyrics/jay/\"\n",
    "ly = \"JayLyrics.txt\"\n",
    "ly_tra = 'JayLyrics_traditional.txt'\n",
    "\n",
    "args = Param()\n",
    "data = DataGenerator(PATH+ly_tra, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 20, 50)            122250    \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 256)               314368    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               128500    \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 2445)              1224945   \n",
      "=================================================================\n",
      "Total params: 2,040,563\n",
      "Trainable params: 2,040,563\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = load_model(\"D:/Program/train_model/lyrics_generator/lyrics_model.h5\")\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# generate a description for an image\n",
    "def generate_desc(model, args, data, in_text):\n",
    "    # seed the generation process\n",
    "\n",
    "    generated = ''\n",
    "    generated += in_text\n",
    "    l = len(in_text)\n",
    "\n",
    "    # iterate over the whole length of the sequence\n",
    "    for i in range(args.lyrics_long):\n",
    "        # integer encode input sequence\n",
    "\n",
    "        in_seq = [data.char2id(c) for c in in_text]\n",
    "\n",
    "        # pad input\n",
    "        sequence = pad_sequences([in_seq], maxlen=data.seq_length, padding='post')\n",
    "        \n",
    "        # predict next word\n",
    "        yhat = model.predict(sequence, verbose=0)\n",
    "        # convert probability to integer\n",
    "        yhat = np.argmax(yhat)\n",
    "        # map integer to word\n",
    "        word = data.id2char(yhat)\n",
    "\n",
    "        # append as input for generating the next word        \n",
    "        generated += word\n",
    "        \n",
    "        if i < data.seq_length-l:\n",
    "            in_text+=word\n",
    "        \n",
    "        else:\n",
    "            in_text = in_text[1:]+word\n",
    "\n",
    "    return generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "你要離開我知道 今天昨天 一起來玩 半獸人 裝撞 \n",
      "天涯颱上學的臉 太多人習慣路 我不會送雙過的姑裝\n",
      "我是一個人事\n",
      "雨過之後更難忘記\n",
      "忘記我還愛你\n",
      "你不用在意\n",
      "流淚也隻是剛好閤意\n",
      "我早已經待在榖底\n",
      "我知道不能再留住你\n",
      "也知道不能沒有孤寂\n",
      "感激你讓我擁有缺點的美麗\n",
      "看著那白色的蜻蜓\n",
      "在空中忘瞭前進\n",
      "還能不能 重新編織\n",
      "腦海中起毛球的記憶\n",
      "再說我愛你\n",
      "可能雨也不會停\n",
      "黑色毛衣\n",
      "藏在音樂頻道\n",
      "離開就讓你道的熱情\n",
      "我的認真敗的消息\n",
      "你會開始學其他同學\n",
      "在書包寫東寫西\n",
      "但我建議最好寫媽媽\n",
      "我會用功讀書\n",
      "用功讀書 怎麼會從我嘴巴說齣\n",
      "不想你輸 所以要叫你用功讀書\n",
      "媽媽織給你的毛衣 你要好好的收著\n",
      "因為母親節到的時候我要告訴她我還留著\n",
      "對瞭我會遇到瞭周潤發\n",
      "所以你可以跟同學炫耀\n",
      "賭神未來是你爸爸\n",
      "我找不到 童年寫的情書\n",
      "你寫完不要送人\n",
      "因為過兩天你還是會把你們當\n"
     ]
    }
   ],
   "source": [
    "in_text = '你要離開我知道 今天昨天 一起來玩 半獸人'\n",
    "\n",
    "gen = generate_desc(model, args, data,in_text)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "讓我們 半獸人 的靈魂翻滾 收起殘忍 迴憶獸化的道\n",
      "而我緊綳的外錶像上緊後的發條\n",
      "等她的答案揭曉\n",
      "她的睫毛彎的嘴角\n",
      "無預警地對我笑\n",
      "沒有預兆齣乎預料\n",
      "竟它在灌木地 誰在閣樓上\n",
      "冰冷的絕望\n",
      "雨輕輕彈\n",
      "硃紅色的窗\n",
      "我一生在紙上\n",
      "被風吹亂\n",
      "夢在遠方\n",
      "化成一縷香\n",
      "隨風飄散你的模樣\n",
      "菊花殘 滿地傷\n",
      "你的笑容勉強不來\n",
      "愛深埋珊瑚鳩\n",
      "引下一整晚\n",
      "你撐把小紙傘 如此溫熱親他\n",
      "動作輕盈地圍繞\n",
      "愛的甜味蔓延發酵\n",
      "曖昧 愛你不捨\n",
      "傻傻的城中\n",
      "吵著吃糖\n",
      "這故事一開始的鏡頭灰塵就已經遮蔽瞭陽光\n",
      "呀 恐懼刻下瞭一個\n",
      "你微笑瀏覽手機裏的浪漫\n",
      "原來愛情可以來得這麼突然\n",
      "短信的橋梁\n",
      "怕你為你不需要我\n",
      "所以你看去的事都有你\n",
      "為你彈奏蕭邦的夜麯\n",
      "紀念我死去的愛情\n",
      "隨著北風\n",
      "微微的笑 赤足又扭腰\n",
      "朝著命運鑿齣的風\n",
      "隻享受到嘴角\n",
      "微微上翹\n",
      "性感地無可救藥\n",
      "想象不到 如此心跳\n",
      "你的一切\n"
     ]
    }
   ],
   "source": [
    "in_text = '讓我們 半獸人 的靈魂翻滾 收起殘忍'\n",
    "\n",
    "gen = generate_desc(model, args, data,in_text)\n",
    "print(gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
